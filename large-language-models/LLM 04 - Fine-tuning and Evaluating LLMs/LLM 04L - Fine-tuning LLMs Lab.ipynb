{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "410d8cf2-52d7-456a-bc9b-064f01e3affa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b149852c-f19c-48c2-9a04-b6a2e2338f77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 04L - Fine-tuning LLMs\n",
    "In this lab, we will apply the fine-tuning learnings from the demo Notebook. The aim of this lab is to fine-tune an instruction-following LLM.\n",
    "\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "1. Prepare a novel dataset\n",
    "1. Fine-tune the T5-small model to classify movie reviews.\n",
    "1. Leverage DeepSpeed to enhance training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ee18f2-0512-4e00-95b6-5fddb15cab21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1289441933715482>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.databricks.clusterUsageTags.sparkVersion\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTHIS LAB REQUIRES THAT A GPU MACHINE AND RUNTIME IS UTILIZED.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\n",
       "\u001B[0;31mAssertionError\u001B[0m: THIS LAB REQUIRES THAT A GPU MACHINE AND RUNTIME IS UTILIZED."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-1289441933715482>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.databricks.clusterUsageTags.sparkVersion\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTHIS LAB REQUIRES THAT A GPU MACHINE AND RUNTIME IS UTILIZED.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\n\u001B[0;31mAssertionError\u001B[0m: THIS LAB REQUIRES THAT A GPU MACHINE AND RUNTIME IS UTILIZED.",
       "errorSummary": "<span class='ansi-red-fg'>AssertionError</span>: THIS LAB REQUIRES THAT A GPU MACHINE AND RUNTIME IS UTILIZED.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert \"gpu\" in spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\"), \"THIS LAB REQUIRES THAT A GPU MACHINE AND RUNTIME IS UTILIZED.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b638d58-ab50-4126-9519-d6d2a0007f68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a19ebc3-37df-48e9-955c-ec907aabe8a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: rouge_score==0.1.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fd50c76e-ca5a-45d4-9c2f-42c6a865ea0f/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: nltk in /databricks/python3/lib/python3.10/site-packages (from rouge_score==0.1.2) (3.7)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.10/site-packages (from rouge_score==0.1.2) (1.21.5)\nRequirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score==0.1.2) (1.16.0)\nRequirement already satisfied: absl-py in /databricks/python3/lib/python3.10/site-packages (from rouge_score==0.1.2) (1.0.0)\nRequirement already satisfied: click in /databricks/python3/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (8.0.4)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (4.64.1)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (1.2.0)\nRequirement already satisfied: regex>=2021.8.3 in /databricks/python3/lib/python3.10/site-packages (from nltk->rouge_score==0.1.2) (2022.7.9)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install rouge_score==0.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6be12e3-accd-4079-9232-a284fc8b2d76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| Enumerating serving endpoints...found 4...(0 seconds)\n| No action taken\n\nSkipping download of existing archive to \"dbfs:/mnt/dbacademy-datasets/large-language-models/v03\" \n| Validating local assets:\n| | Listing local files...(0 seconds)\n| | Validation completed...(0 seconds total)\n|\n| Skipping the unpacking of datasets to \"dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets\" \n|\n| Dataset installation completed (0 seconds)\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing lab testing framework.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nUsing the \"default\" schema.\n\nPredefined paths variables:\n| DA.paths.working_dir: /dbfs/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/working\n| DA.paths.user_db:     /dbfs/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/working/database.db\n| DA.paths.datasets:    /dbfs/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets\n\nSetup completed (5 seconds)\n\nThe models developed or used in this course are for demonstration and learning purposes only.\nModels may occasionally output offensive, inaccurate, biased information, or harmful instructions.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c692d504-d71b-4789-abf7-d1720748cb58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:          johnlennyt@gmail.com\nWorking Directory: /dbfs/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/working\n"
     ]
    }
   ],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1b990ae-b5ed-4d42-ba97-d81ab0894af4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d56e2022-f58b-4275-a5e2-1d4540599423",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creating a local temporary directory on the Driver. This will serve as a root directory for the intermediate model checkpoints created during the training process. The final model will be persisted to DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2088db44-236a-425a-8d55-8a70472f24f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "tmpdir = tempfile.TemporaryDirectory()\n",
    "local_training_root = tmpdir.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "072b8a30-eeaf-4a75-ab6c-28cd7ba11641",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d14c20-74af-4c78-a73a-bfc0f42d4ab5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import transformers as tr\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Trainer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4ef3070-eaf4-48e2-a3ab-66f8f9aa924e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 1: Data Preparation\n",
    "For the instruction-following use cases we need a dataset that consists of prompt/response pairs along with any contextual information that can be used as input when training the model. The [databricks/databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) is one such dataset that provides high-quality, human-generated prompt/response pairs. \n",
    "\n",
    "Let's start by loading this dataset using the `load_dataset` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d064bc8-3869-4aa2-be6c-3135b2ba9793",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:27: UserWarning: This dataset can not be stored in DBFS because either `cache_dir` or the environment variable `HF_DATASETS_CACHE` is set to a non-DBFS path. If this cluster restarts, all saved dataset information will be lost.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516463e85e7a433faf51f5c125d9e2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:13: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/databricks--databricks-dolly-15k to /root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba86c2d3bfe84c3284bd4ebd52fe3d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff669e30fa284608a90474ede0bcf4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c324ffe1c5040ec99b97b9d640e18b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec359a6ba5e4eae818afbae97da97a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586cbbf72f774c73a595be98907aff56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "ds = load_dataset(\"databricks/databricks-dolly-15k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb27e372-465f-4d6a-b5a8-ac969728af18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson4, question1\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_1(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dca2a2ee-a2ee-4f78-909e-4daae5a2931c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 2: Select pre-trained model\n",
    "\n",
    "The model that we are going to fine-tune is [pythia-70m-deduped](https://huggingface.co/EleutherAI/pythia-70m-deduped). This model is one of a Pythia Suite of models that have been developed to support interpretability research.\n",
    "\n",
    "Let's define the pre-trained model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "458a620f-df9b-4586-8f5a-1911ffe02958",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "model_checkpoint = \"EleutherAI/pythia-70m-deduped\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e32688a-de44-45c4-be8e-b6e749dd712b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson4, question2\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_2(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a75cd9de-aadf-4c05-a058-4f64a0503d4f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 3: Load and Configure\n",
    "\n",
    "The next task is to load and configure the tokenizer for this model. The instruction-following process builds a body of text that contains the instruction, context input, and response values from the dataset. The body of text also includes some special tokens to identify the sections of the text. These tokens are generally configurable, and need to be added to the tokenizer.\n",
    "\n",
    "Let's go ahead and load the tokenizer for the pre-trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98856f28-a982-4d2b-8fb1-b7b57470f95b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "# load the tokenizer that was used for the model\n",
    "tokenizer = tr.AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": [\"### End\", \"### Instruction:\", \"### Response:\\n\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331876de-e2cd-4ddf-9029-df9ec4a7544a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson4, question3\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_3(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22556a86-a117-4271-9e47-d47789118f9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 4: Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1752e428-cb55-4b75-bcca-51050a689878",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The `tokenize` method below builds the body of text for each prompt/response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429ced86-ff30-4a61-a124-b321d396ed05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "remove_columns = [\"instruction\", \"response\", \"context\", \"category\"]\n",
    "\n",
    "\n",
    "def tokenize(x: dict, max_length: int = 1024) -> dict:\n",
    "    \"\"\"\n",
    "    For a dictionary example of instruction, response, and context a dictionary of input_id and attention mask is returned\n",
    "    \"\"\"\n",
    "    instr = x[\"instruction\"]\n",
    "    resp = x[\"response\"]\n",
    "    context = x[\"context\"]\n",
    "\n",
    "    instr_part = f\"### Instruction:\\n{instr}\"\n",
    "    context_part = \"\"\n",
    "    if context:\n",
    "        context_part = f\"\\nInput:\\n{context}\\n\"\n",
    "    resp_part = f\"### Response:\\n{resp}\"\n",
    "\n",
    "    text = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "{instr_part}\n",
    "{context_part}\n",
    "{resp_part}\n",
    "\n",
    "### End\n",
    "\"\"\"\n",
    "    return tokenizer(text, max_length=max_length, truncation=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7391cd31-d598-4ba5-9f01-b767d6c1e0c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's `tokenize` the Dolly training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bbfa78f-8a40-4797-bcd9-953b52f1b2c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c935c1b629e40be9b2777d709fda618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "tokenized_dataset = ds.map(\n",
    "    tokenize, batched=True, remove_columns=remove_columns\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d681792c-5f46-4c4a-ac45-914a942151d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1289441933715506>, line 3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Test your answer. DO NOT MODIFY THIS CELL.\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m \u001B[43mdbTestQuestion4_4\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenized_dataset\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m<command-1289441933715840>, line 32\u001B[0m, in \u001B[0;36mdbTestQuestion4_4\u001B[0;34m(tokenized_dataset)\u001B[0m\n",
       "\u001B[1;32m     29\u001B[0m userhome_for_testing \u001B[38;5;241m=\u001B[39m getUsernameFromEnv(lesson)\n",
       "\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mtype\u001B[39m(tokenized_dataset)) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<class \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdatasets.dataset_dict.DatasetDict\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m>\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest NOT passed: `tokenized_dataset` should be of type `datasets.dataset_dict.DatasetDict`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m---> 32\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m  \u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtokenized_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(tokenized_dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m]), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest NOT passed: For each entry the number of `input_ids` and `attention_masks` should be equal\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     34\u001B[0m questionPassed(userhome_for_testing, lesson, question)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: object of type 'int' has no len()"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1289441933715506>, line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Test your answer. DO NOT MODIFY THIS CELL.\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mdbTestQuestion4_4\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenized_dataset\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m<command-1289441933715840>, line 32\u001B[0m, in \u001B[0;36mdbTestQuestion4_4\u001B[0;34m(tokenized_dataset)\u001B[0m\n\u001B[1;32m     29\u001B[0m userhome_for_testing \u001B[38;5;241m=\u001B[39m getUsernameFromEnv(lesson)\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mtype\u001B[39m(tokenized_dataset)) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<class \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdatasets.dataset_dict.DatasetDict\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m>\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest NOT passed: `tokenized_dataset` should be of type `datasets.dataset_dict.DatasetDict`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 32\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m  \u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtokenized_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(tokenized_dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m]), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest NOT passed: For each entry the number of `input_ids` and `attention_masks` should be equal\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     34\u001B[0m questionPassed(userhome_for_testing, lesson, question)\n\n\u001B[0;31mTypeError\u001B[0m: object of type 'int' has no len()",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: object of type 'int' has no len()",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_4(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16dea318-a791-43bd-a6a0-ec546f29cb4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 5: Setup Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54d11da2-8f7f-494e-9d05-bf49ecc4afa9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To setup the fine-tuning process we need to define the `TrainingArguments`.\n",
    "\n",
    "Let's configure the training to have **10** training epochs (`num_train_epochs`) with a per device batch size of **8**. The optimizer (`optim`) to be used should be `adamw_torch`. Finally, the reporting (`report_to`) list should be set to *tensorboard*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7c3a948-7ea4-441d-84b4-4c6f5703aeae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1289441933715509>, line 4\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m checkpoint_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest-trainer-lab\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      3\u001B[0m local_checkpoint_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(local_training_root, checkpoint_name)\n",
       "\u001B[0;32m----> 4\u001B[0m training_args \u001B[38;5;241m=\u001B[39m tr\u001B[38;5;241m.\u001B[39mTrainingArguments(\n",
       "\u001B[1;32m      5\u001B[0m     local_checkpoint_path,\n",
       "\u001B[1;32m      6\u001B[0m     num_train_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,  \u001B[38;5;66;03m# default number of epochs to train is 3\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m     per_device_train_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m,\n",
       "\u001B[1;32m      8\u001B[0m     optim\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madamw_torch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      9\u001B[0m     report_to\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtensorboard\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n",
       "\u001B[1;32m     10\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m<string>:111\u001B[0m, in \u001B[0;36m__init__\u001B[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/training_args.py:1340\u001B[0m, in \u001B[0;36mTrainingArguments.__post_init__\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1334\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m version\u001B[38;5;241m.\u001B[39mparse(version\u001B[38;5;241m.\u001B[39mparse(torch\u001B[38;5;241m.\u001B[39m__version__)\u001B[38;5;241m.\u001B[39mbase_version) \u001B[38;5;241m==\u001B[39m version\u001B[38;5;241m.\u001B[39mparse(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.0.0\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16:\n",
       "\u001B[1;32m   1335\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1337\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n",
       "\u001B[1;32m   1338\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1339\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n",
       "\u001B[0;32m-> 1340\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1341\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (get_xla_device_type(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGPU\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1342\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16_full_eval)\n",
       "\u001B[1;32m   1343\u001B[0m ):\n",
       "\u001B[1;32m   1344\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m   1345\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1346\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (`--fp16_full_eval`) can only be used on CUDA devices.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1347\u001B[0m     )\n",
       "\u001B[1;32m   1349\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1351\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1356\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16_full_eval)\n",
       "\u001B[1;32m   1357\u001B[0m ):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/training_args.py:1764\u001B[0m, in \u001B[0;36mTrainingArguments.device\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1760\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1761\u001B[0m \u001B[38;5;124;03mThe device used by this process.\u001B[39;00m\n",
       "\u001B[1;32m   1762\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1763\u001B[0m requires_backends(\u001B[38;5;28mself\u001B[39m, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
       "\u001B[0;32m-> 1764\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_devices\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/utils/generic.py:54\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[0;34m(self, obj, objtype)\u001B[0m\n",
       "\u001B[1;32m     52\u001B[0m cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(obj, attr, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
       "\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cached \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m---> 54\u001B[0m     cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     55\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(obj, attr, cached)\n",
       "\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cached\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/training_args.py:1695\u001B[0m, in \u001B[0;36mTrainingArguments._setup_devices\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1693\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_gpu \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m   1694\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1695\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistributed_state \u001B[38;5;241m=\u001B[39m \u001B[43mPartialState\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbackend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mddp_backend\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1696\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_gpu \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m   1697\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sagemaker_mp_enabled():\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/accelerate/state.py:191\u001B[0m, in \u001B[0;36mPartialState.__init__\u001B[0;34m(self, cpu, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    189\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    190\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnccl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m--> 191\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistributed\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit_process_group\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbackend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackend\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    192\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_processes \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdistributed\u001B[38;5;241m.\u001B[39mget_world_size()\n",
       "\u001B[1;32m    193\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_index \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdistributed\u001B[38;5;241m.\u001B[39mget_rank()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:761\u001B[0m, in \u001B[0;36minit_process_group\u001B[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001B[0m\n",
       "\u001B[1;32m    757\u001B[0m         \u001B[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001B[39;00m\n",
       "\u001B[1;32m    758\u001B[0m         \u001B[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001B[39;00m\n",
       "\u001B[1;32m    759\u001B[0m         store \u001B[38;5;241m=\u001B[39m PrefixStore(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdefault_pg\u001B[39m\u001B[38;5;124m\"\u001B[39m, store)\n",
       "\u001B[0;32m--> 761\u001B[0m     default_pg \u001B[38;5;241m=\u001B[39m \u001B[43m_new_process_group_helper\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    762\u001B[0m \u001B[43m        \u001B[49m\u001B[43mworld_size\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    763\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    764\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    765\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbackend\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    766\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    767\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpg_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpg_options\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    768\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgroup_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup_name\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    769\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    770\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    771\u001B[0m     _update_default_pg(default_pg)\n",
       "\u001B[1;32m    773\u001B[0m _pg_group_ranks[GroupMember\u001B[38;5;241m.\u001B[39mWORLD] \u001B[38;5;241m=\u001B[39m {i: i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(GroupMember\u001B[38;5;241m.\u001B[39mWORLD\u001B[38;5;241m.\u001B[39msize())}  \u001B[38;5;66;03m# type: ignore[attr-defined, index]\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:886\u001B[0m, in \u001B[0;36m_new_process_group_helper\u001B[0;34m(group_size, group_rank, global_ranks_in_group, backend, store, pg_options, group_name, timeout)\u001B[0m\n",
       "\u001B[1;32m    884\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m backend \u001B[38;5;241m==\u001B[39m Backend\u001B[38;5;241m.\u001B[39mNCCL:\n",
       "\u001B[1;32m    885\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_nccl_available():\n",
       "\u001B[0;32m--> 886\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDistributed package doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt have NCCL \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuilt in\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    887\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m pg_options \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    888\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n",
       "\u001B[1;32m    889\u001B[0m             pg_options, ProcessGroupNCCL\u001B[38;5;241m.\u001B[39mOptions\n",
       "\u001B[1;32m    890\u001B[0m         ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected pg_options argument to be of type ProcessGroupNCCL.Options\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\n",
       "\u001B[0;31mRuntimeError\u001B[0m: Distributed package doesn't have NCCL built in"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)\nFile \u001B[0;32m<command-1289441933715509>, line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m checkpoint_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest-trainer-lab\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      3\u001B[0m local_checkpoint_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(local_training_root, checkpoint_name)\n\u001B[0;32m----> 4\u001B[0m training_args \u001B[38;5;241m=\u001B[39m tr\u001B[38;5;241m.\u001B[39mTrainingArguments(\n\u001B[1;32m      5\u001B[0m     local_checkpoint_path,\n\u001B[1;32m      6\u001B[0m     num_train_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,  \u001B[38;5;66;03m# default number of epochs to train is 3\u001B[39;00m\n\u001B[1;32m      7\u001B[0m     per_device_train_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m,\n\u001B[1;32m      8\u001B[0m     optim\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madamw_torch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      9\u001B[0m     report_to\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtensorboard\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     10\u001B[0m )\n\nFile \u001B[0;32m<string>:111\u001B[0m, in \u001B[0;36m__init__\u001B[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001B[0m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/training_args.py:1340\u001B[0m, in \u001B[0;36mTrainingArguments.__post_init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1334\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m version\u001B[38;5;241m.\u001B[39mparse(version\u001B[38;5;241m.\u001B[39mparse(torch\u001B[38;5;241m.\u001B[39m__version__)\u001B[38;5;241m.\u001B[39mbase_version) \u001B[38;5;241m==\u001B[39m version\u001B[38;5;241m.\u001B[39mparse(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.0.0\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16:\n\u001B[1;32m   1335\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1337\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1338\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1339\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n\u001B[0;32m-> 1340\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1341\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (get_xla_device_type(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGPU\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1342\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16_full_eval)\n\u001B[1;32m   1343\u001B[0m ):\n\u001B[1;32m   1344\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1345\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1346\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (`--fp16_full_eval`) can only be used on CUDA devices.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1347\u001B[0m     )\n\u001B[1;32m   1349\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1351\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1356\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16_full_eval)\n\u001B[1;32m   1357\u001B[0m ):\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/training_args.py:1764\u001B[0m, in \u001B[0;36mTrainingArguments.device\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1760\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1761\u001B[0m \u001B[38;5;124;03mThe device used by this process.\u001B[39;00m\n\u001B[1;32m   1762\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1763\u001B[0m requires_backends(\u001B[38;5;28mself\u001B[39m, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m-> 1764\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_devices\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/utils/generic.py:54\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[0;34m(self, obj, objtype)\u001B[0m\n\u001B[1;32m     52\u001B[0m cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(obj, attr, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cached \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 54\u001B[0m     cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(obj, attr, cached)\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cached\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/transformers/training_args.py:1695\u001B[0m, in \u001B[0;36mTrainingArguments._setup_devices\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1693\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_gpu \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1694\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1695\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistributed_state \u001B[38;5;241m=\u001B[39m \u001B[43mPartialState\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbackend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mddp_backend\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1696\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_gpu \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1697\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sagemaker_mp_enabled():\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/accelerate/state.py:191\u001B[0m, in \u001B[0;36mPartialState.__init__\u001B[0;34m(self, cpu, **kwargs)\u001B[0m\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    190\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnccl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 191\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistributed\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit_process_group\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbackend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackend\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_processes \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdistributed\u001B[38;5;241m.\u001B[39mget_world_size()\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_index \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdistributed\u001B[38;5;241m.\u001B[39mget_rank()\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:761\u001B[0m, in \u001B[0;36minit_process_group\u001B[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001B[0m\n\u001B[1;32m    757\u001B[0m         \u001B[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001B[39;00m\n\u001B[1;32m    758\u001B[0m         \u001B[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001B[39;00m\n\u001B[1;32m    759\u001B[0m         store \u001B[38;5;241m=\u001B[39m PrefixStore(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdefault_pg\u001B[39m\u001B[38;5;124m\"\u001B[39m, store)\n\u001B[0;32m--> 761\u001B[0m     default_pg \u001B[38;5;241m=\u001B[39m \u001B[43m_new_process_group_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    762\u001B[0m \u001B[43m        \u001B[49m\u001B[43mworld_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    763\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    764\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    765\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbackend\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    766\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    767\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpg_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpg_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    768\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgroup_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    769\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    770\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    771\u001B[0m     _update_default_pg(default_pg)\n\u001B[1;32m    773\u001B[0m _pg_group_ranks[GroupMember\u001B[38;5;241m.\u001B[39mWORLD] \u001B[38;5;241m=\u001B[39m {i: i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(GroupMember\u001B[38;5;241m.\u001B[39mWORLD\u001B[38;5;241m.\u001B[39msize())}  \u001B[38;5;66;03m# type: ignore[attr-defined, index]\u001B[39;00m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:886\u001B[0m, in \u001B[0;36m_new_process_group_helper\u001B[0;34m(group_size, group_rank, global_ranks_in_group, backend, store, pg_options, group_name, timeout)\u001B[0m\n\u001B[1;32m    884\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m backend \u001B[38;5;241m==\u001B[39m Backend\u001B[38;5;241m.\u001B[39mNCCL:\n\u001B[1;32m    885\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_nccl_available():\n\u001B[0;32m--> 886\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDistributed package doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt have NCCL \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuilt in\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    887\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m pg_options \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    888\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m    889\u001B[0m             pg_options, ProcessGroupNCCL\u001B[38;5;241m.\u001B[39mOptions\n\u001B[1;32m    890\u001B[0m         ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected pg_options argument to be of type ProcessGroupNCCL.Options\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\n\u001B[0;31mRuntimeError\u001B[0m: Distributed package doesn't have NCCL built in",
       "errorSummary": "<span class='ansi-red-fg'>RuntimeError</span>: Distributed package doesn't have NCCL built in",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "checkpoint_name = \"test-trainer-lab\"\n",
    "local_checkpoint_path = os.path.join(local_training_root, checkpoint_name)\n",
    "training_args = tr.TrainingArguments(\n",
    "    local_checkpoint_path,\n",
    "    num_train_epochs=10,  # default number of epochs to train is 3\n",
    "    per_device_train_batch_size=8,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=[\"tensorboard\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d7c226-320b-46e0-9926-1b0e7c4f9ee9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_name = \"test-trainer-lab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d9d3c82-b078-4800-814b-8ae412ddfd1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1289441933715511>, line 3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Test your answer. DO NOT MODIFY THIS CELL.\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m dbTestQuestion4_5(\u001B[43mtraining_args\u001B[49m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'training_args' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1289441933715511>, line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Test your answer. DO NOT MODIFY THIS CELL.\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m dbTestQuestion4_5(\u001B[43mtraining_args\u001B[49m)\n\n\u001B[0;31mNameError\u001B[0m: name 'training_args' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'training_args' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_5(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b1478a3-272d-4848-aea3-067142e49967",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 6: AutoModelForCausalLM\n",
    "\n",
    "The pre-trained `pythia-70m-deduped` model can be loaded using the [AutoModelForCausalLM](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdbd34f1-6942-4f83-8a1a-5280244984cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# load the pre-trained model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c273a3-48be-4a75-82e9-5e88ed919519",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson4, question6\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_6(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6929a25d-9d3c-4497-b89e-d378e9f76789",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 7: Initialize the Trainer\n",
    "\n",
    "Unlike the IMDB dataset used in the earlier Notebook, the Dolly dataset only contains a single *train* dataset. Let's go ahead and create a [`train_test_split`](https://huggingface.co/docs/datasets/v2.12.0/en/package_reference/main_classes#datasets.Dataset.train_test_split) of the train dataset.\n",
    "\n",
    "Also, let's initialize the [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) with model, training arguments, the train & test datasets, tokenizer, and data collator. Here we will use the [`DataCollatorForLanguageModeling`](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47e21a34-df36-4f82-aa8b-03688912da49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1289441933715516>, line 9\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m SEED\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m\n",
       "\u001B[1;32m      5\u001B[0m data_collator \u001B[38;5;241m=\u001B[39m DataCollatorForLanguageModeling(\n",
       "\u001B[1;32m      6\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mtokenizer, mlm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, pad_to_multiple_of\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m\n",
       "\u001B[1;32m      7\u001B[0m )\n",
       "\u001B[0;32m----> 9\u001B[0m split_dataset \u001B[38;5;241m=\u001B[39m tokenized_dataset\u001B[38;5;241m.\u001B[39mtrain_test_split(test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, seed\u001B[38;5;241m=\u001B[39mSEED)\n",
       "\u001B[1;32m     11\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n",
       "\u001B[1;32m     12\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n",
       "\u001B[1;32m     13\u001B[0m     args\u001B[38;5;241m=\u001B[39mTrainingArguments(\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     23\u001B[0m     eval_dataset\u001B[38;5;241m=\u001B[39msplit_dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n",
       "\u001B[1;32m     24\u001B[0m )\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'DatasetDict' object has no attribute 'train_test_split'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-1289441933715516>, line 9\u001B[0m\n\u001B[1;32m      4\u001B[0m SEED\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m\n\u001B[1;32m      5\u001B[0m data_collator \u001B[38;5;241m=\u001B[39m DataCollatorForLanguageModeling(\n\u001B[1;32m      6\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mtokenizer, mlm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, pad_to_multiple_of\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m\n\u001B[1;32m      7\u001B[0m )\n\u001B[0;32m----> 9\u001B[0m split_dataset \u001B[38;5;241m=\u001B[39m tokenized_dataset\u001B[38;5;241m.\u001B[39mtrain_test_split(test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, seed\u001B[38;5;241m=\u001B[39mSEED)\n\u001B[1;32m     11\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m     12\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     13\u001B[0m     args\u001B[38;5;241m=\u001B[39mTrainingArguments(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     23\u001B[0m     eval_dataset\u001B[38;5;241m=\u001B[39msplit_dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     24\u001B[0m )\n\n\u001B[0;31mAttributeError\u001B[0m: 'DatasetDict' object has no attribute 'train_test_split'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'DatasetDict' object has no attribute 'train_test_split'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "# used to assist the trainer in batching the data\n",
    "TRAINING_SIZE=6000\n",
    "SEED=42\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=SEED)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./your_output_directory\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "    ),\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f12cdd8-42d2-4a1f-b955-723c0d4a73cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_7(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b197f7e-373f-4308-877c-85f0b05e0e19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 8: Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca967bbb-1c15-4d61-b701-cc6f271d1762",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Before starting the training process, let's turn on Tensorboard. This will allow us to monitor the training process as checkpoint logs are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69ce7a9e-02a2-4e9a-9b88-64bd429cf539",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tensorboard_display_dir = f\"{local_checkpoint_path}/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bf9980b-338f-419f-b423-35586d0baa60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{tensorboard_display_dir}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e52bae6-ea33-4137-9be4-e5d4be6c3f46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Start the fine-tuning process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b596c8bd-e7a2-4600-a56d-199bdc521093",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# invoke training - note this will take approx. 30min\n",
    "trainer.train\n",
    "\n",
    "# save model to the local checkpoint\n",
    "trainer.save_model()\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b1d1fc2-d426-4d1f-8b68-f6e600db7c41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_8(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65f33806-ed36-4a62-b466-2f63660c1c09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# persist the fine-tuned model to DBFS\n",
    "final_model_path = f\"{DA.paths.working_dir}/llm04_fine_tuning/{checkpoint_name}\"\n",
    "trainer.save_model(output_dir=final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1bc28a8-6cae-4d4b-9a2d-22b5f68718cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dba503f9-0f2a-4b46-af2d-b30d533c947b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfc3c5c9-fea4-4c53-9cf9-19cae354f107",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Recall that the model was trained using a body of text that contained an instruction and its response. A similar body of text, or prompt, needs to be provided when testing the model. The prompt that is provided only contains an instruction though. The model will `generate` the response accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4d701da-52e4-4228-b147-f313c131a6f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_prompt(instr: str, max_length: int = 1024) -> dict:\n",
    "    text = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instr}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    return tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "\n",
    "\n",
    "def to_response(prediction):\n",
    "    decoded = tokenizer.decode(prediction)\n",
    "    # extract the Response from the decoded sequence\n",
    "    m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\s*End\", decoded, flags=re.DOTALL)\n",
    "    res = \"Failed to find response\"\n",
    "    if m:\n",
    "        res = m.group(1).strip()\n",
    "    else:\n",
    "        m = re.search(r\"#+\\s*Response:\\s*(.+)\", decoded, flags=re.DOTALL)\n",
    "        if m:\n",
    "            res = m.group(1).strip()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "811a7ce3-c2c9-416b-a5c1-5d7e6d21b704",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# NOTE: this cell can take up to 5mins\n",
    "res = []\n",
    "for i in range(100):\n",
    "    instr = ds[\"train\"][i][\"instruction\"]\n",
    "    resp = ds[\"train\"][i][\"response\"]\n",
    "    inputs = to_prompt(instr)\n",
    "    pred = fine_tuned_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "    res.append((instr, resp, to_response(pred[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "268d61c9-5193-4e49-b70f-abf94608ec1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(res, columns=[\"instruction\", \"response\", \"generated\"])\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcb00cf4-4e25-4754-8751-09337036bb53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**CONGRATULATIONS**\n",
    "\n",
    "You have just taken the first step toward fine-tuning your own slimmed down version of [Dolly](https://github.com/databrickslabs/dolly)! \n",
    "\n",
    "Unfortunately, it does not seem to be too generative at the moment. Perhaps, with some additional training and data the model could be more capable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeec687a-81c7-473b-824d-3619d9b26927",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Question 9: Evaluation\n",
    "\n",
    "Although the current model is under-trained, it is worth evaluating the responses to get a general sense of how far off the model is at this point.\n",
    "\n",
    "Let's compute the ROGUE metrics between the reference response and the generated responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d873aa37-8a6d-4cda-9a0b-892f1cd00182",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "def compute_rouge_score(generated, reference):\n",
    "    \"\"\"\n",
    "    Compute ROUGE scores on a batch of articles.\n",
    "\n",
    "    This is a convenience function wrapping Hugging Face `rouge_score`,\n",
    "    which expects sentences to be separated by newlines.\n",
    "\n",
    "    :param generated: Summaries (list of strings) produced by the model\n",
    "    :param reference: Ground-truth summaries (list of strings) for comparison\n",
    "    \"\"\"\n",
    "    generated_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in generated]\n",
    "    reference_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in reference]\n",
    "    return rouge_score.compute(\n",
    "        predictions=generated_with_newlines,\n",
    "        references=reference_with_newlines,\n",
    "        use_stemmer=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ddf629-623a-472e-b5cb-d4e32b722dc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1289441933715535>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# TODO\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m rouge_scores \u001B[38;5;241m=\u001B[39m compute_rouge_score(generated, reference)\n",
       "\u001B[1;32m      3\u001B[0m display(rouge_scores)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'generated' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1289441933715535>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# TODO\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m rouge_scores \u001B[38;5;241m=\u001B[39m compute_rouge_score(generated, reference)\n\u001B[1;32m      3\u001B[0m display(rouge_scores)\n\n\u001B[0;31mNameError\u001B[0m: name 'generated' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'generated' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "rouge_scores = compute_rouge_score(generated, reference)\n",
    "display(rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb11e41-38a7-495e-a264-ddff61aecfbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1289441933715536>, line 3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Test your answer. DO NOT MODIFY THIS CELL.\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m dbTestQuestion4_9(\u001B[43mrouge_scores\u001B[49m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'rouge_scores' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1289441933715536>, line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Test your answer. DO NOT MODIFY THIS CELL.\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m dbTestQuestion4_9(\u001B[43mrouge_scores\u001B[49m)\n\n\u001B[0;31mNameError\u001B[0m: name 'rouge_scores' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'rouge_scores' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion4_9(rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de450786-23b4-46b7-986e-ba62272681cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Clean up Classroom\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fa79e61-e7f8-4cc6-b97d-ff53b4ebd87d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tmpdir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cffa29a-dfd5-4769-a72e-ff29701bbf5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Submit your Results (edX Verified Only)\n",
    "\n",
    "To get credit for this lab, click the submit button in the top right to report the results. If you run into any issues, click `Run` -> `Clear state and run all`, and make sure all tests have passed before re-submitting. If you accidentally deleted any tests, take a look at the notebook's version history to recover them or reload the notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d814208-3073-4dbe-81ae-4a6e27878b38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LLM 04L - Fine-tuning LLMs Lab",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
