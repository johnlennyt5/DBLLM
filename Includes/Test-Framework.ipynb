{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57c73ad5-b97d-468d-bf71-e74e9f65c2c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing lab testing framework.\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing lab testing framework.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b4236cc-82a3-403e-ac2b-bb177f8c8b77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1289441933715836>, line 46\u001B[0m, in \u001B[0;36mgetUsernameFromEnv\u001B[0;34m(lesson)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 46\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mDA\u001B[49m\u001B[38;5;241m.\u001B[39mpaths\u001B[38;5;241m.\u001B[39mworking_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-testing-files/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlesson\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNameError\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'DA' is not defined\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1289441933715836>, line 50\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNameError\u001B[39;00m:\n",
       "\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNameError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWorking directory not found. Please re-run the Classroom-Setup at the beginning of the notebook.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 50\u001B[0m createDirStructure()\n",
       "\n",
       "File \u001B[0;32m<command-1289441933715836>, line 18\u001B[0m, in \u001B[0;36mcreateDirStructure\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n",
       "\u001B[1;32m     11\u001B[0m lesson_question_d \u001B[38;5;241m=\u001B[39m {\n",
       "\u001B[1;32m     12\u001B[0m   \u001B[38;5;241m1\u001B[39m: \u001B[38;5;241m3\u001B[39m, \u001B[38;5;66;03m# TODO: confirm these questions once tests are finalized \u001B[39;00m\n",
       "\u001B[1;32m     13\u001B[0m   \u001B[38;5;241m2\u001B[39m: \u001B[38;5;241m6\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     16\u001B[0m   \u001B[38;5;241m5\u001B[39m: \u001B[38;5;241m5\u001B[39m,\n",
       "\u001B[1;32m     17\u001B[0m }\n",
       "\u001B[0;32m---> 18\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[43mgetUsernameFromEnv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m lesson, questions \u001B[38;5;129;01min\u001B[39;00m lesson_question_d\u001B[38;5;241m.\u001B[39mitems():\n",
       "\u001B[1;32m     21\u001B[0m   \u001B[38;5;28;01mfor\u001B[39;00m question \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, questions\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m<command-1289441933715836>, line 48\u001B[0m, in \u001B[0;36mgetUsernameFromEnv\u001B[0;34m(lesson)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mDA\u001B[38;5;241m.\u001B[39mpaths\u001B[38;5;241m.\u001B[39mworking_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-testing-files/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlesson\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNameError\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNameError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWorking directory not found. Please re-run the Classroom-Setup at the beginning of the notebook.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: Working directory not found. Please re-run the Classroom-Setup at the beginning of the notebook."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1289441933715836>, line 46\u001B[0m, in \u001B[0;36mgetUsernameFromEnv\u001B[0;34m(lesson)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 46\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mDA\u001B[49m\u001B[38;5;241m.\u001B[39mpaths\u001B[38;5;241m.\u001B[39mworking_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-testing-files/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlesson\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNameError\u001B[39;00m:\n\n\u001B[0;31mNameError\u001B[0m: name 'DA' is not defined\n\nDuring handling of the above exception, another exception occurred:\n\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1289441933715836>, line 50\u001B[0m\n\u001B[1;32m     47\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNameError\u001B[39;00m:\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNameError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWorking directory not found. Please re-run the Classroom-Setup at the beginning of the notebook.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 50\u001B[0m createDirStructure()\n\nFile \u001B[0;32m<command-1289441933715836>, line 18\u001B[0m, in \u001B[0;36mcreateDirStructure\u001B[0;34m()\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[1;32m     11\u001B[0m lesson_question_d \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     12\u001B[0m   \u001B[38;5;241m1\u001B[39m: \u001B[38;5;241m3\u001B[39m, \u001B[38;5;66;03m# TODO: confirm these questions once tests are finalized \u001B[39;00m\n\u001B[1;32m     13\u001B[0m   \u001B[38;5;241m2\u001B[39m: \u001B[38;5;241m6\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     16\u001B[0m   \u001B[38;5;241m5\u001B[39m: \u001B[38;5;241m5\u001B[39m,\n\u001B[1;32m     17\u001B[0m }\n\u001B[0;32m---> 18\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[43mgetUsernameFromEnv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m lesson, questions \u001B[38;5;129;01min\u001B[39;00m lesson_question_d\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m     21\u001B[0m   \u001B[38;5;28;01mfor\u001B[39;00m question \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, questions\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\nFile \u001B[0;32m<command-1289441933715836>, line 48\u001B[0m, in \u001B[0;36mgetUsernameFromEnv\u001B[0;34m(lesson)\u001B[0m\n\u001B[1;32m     46\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mDA\u001B[38;5;241m.\u001B[39mpaths\u001B[38;5;241m.\u001B[39mworking_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-testing-files/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlesson\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNameError\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNameError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWorking directory not found. Please re-run the Classroom-Setup at the beginning of the notebook.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\n\u001B[0;31mNameError\u001B[0m: Working directory not found. Please re-run the Classroom-Setup at the beginning of the notebook.",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: Working directory not found. Please re-run the Classroom-Setup at the beginning of the notebook.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DEFINING HELPER FUNCTIONS\n",
    "\n",
    "def createDirStructure():\n",
    "  '''\n",
    "  This creates the directories needed for the test handler.\n",
    "  Note that `lesson_question_d` is lesson num: num of questions.\n",
    "    Modify this when changing the number of questions\n",
    "  '''\n",
    "  from pathlib import Path\n",
    "\n",
    "  lesson_question_d = {\n",
    "    1: 3, # TODO: confirm these questions once tests are finalized \n",
    "    2: 6,\n",
    "    3: 4,\n",
    "    4: 9,\n",
    "    5: 5,\n",
    "  }\n",
    "  path = getUsernameFromEnv(\"\")\n",
    "\n",
    "  for lesson, questions in lesson_question_d.items():\n",
    "    for question in range(1, questions+1):\n",
    "      final_path = f\"{path}lesson{lesson}/question{question}\"\n",
    "      Path(final_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def questionPassed(userhome_for_testing, lesson, question):\n",
    "  '''\n",
    "  Helper function that writes an empty file named `PASSED` to the designated path\n",
    "  '''\n",
    "  from pathlib import Path\n",
    "\n",
    "  print(f\"\\u001b[32mPASSED\\x1b[0m: All tests passed for {lesson}, {question}\")\n",
    "\n",
    "  path = f\"{userhome_for_testing}/{question}\"\n",
    "  Path(path).mkdir(parents=True, exist_ok=True)\n",
    "  with open(f\"{path}/PASSED\", \"wb\") as handle:\n",
    "      pass # just write an empty file\n",
    "  \n",
    "  print (\"\\u001b[32mRESULTS RECORDED\\x1b[0m: Click `Submit` when all questions are completed to log the results.\")\n",
    "\n",
    "def getUsernameFromEnv(lesson):\n",
    "  '''\n",
    "  Exception handling for when the working directory is not in the scope\n",
    "  (i.e. the Classroom-Setup was not run)\n",
    "  '''\n",
    "  try:\n",
    "    return f\"{DA.paths.working_dir}-testing-files/{lesson}\"\n",
    "  except NameError:\n",
    "    raise NameError(\"Working directory not found. Please re-run the Classroom-Setup at the beginning of the notebook.\")\n",
    "\n",
    "createDirStructure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e9b9208-75b9-4de3-9623-4103c57652af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LLM 01L - LLMs with Hugging Face Lab\n",
    "\n",
    "def dbTestQuestion1_1(summarizer, summarization_results, summarizer_inputs):\n",
    "  lesson, question = \"lesson1\", \"question1\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert str(summarizer.task) == \"summarization\", \"Test NOT passed: Pipeline should be built for task `summarization`\"\n",
    "  assert isinstance(summarization_results, list), \"Test NOT passed: Result should be a list.\"\n",
    "  assert len(summarization_results) == len(summarizer_inputs), \"Test NOT passed: Result should be a list of length equal to the input dataset size.\"\n",
    "  assert min([len(s) for s in summarization_results]) > 0, \"Test NOT passed: Summaries should be non-empty.\"\n",
    "\n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "def dbTestQuestion1_2(translation_pipeline, translation_results, translation_inputs):\n",
    "  lesson, question = \"lesson1\", \"question2\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert \"translation\" in str(translation_pipeline.task), \"Test NOT passed: Pipeline should be built for task `translation`\"\n",
    "  assert isinstance(translation_results, list), \"Test NOT passed: Result should be a list.\"\n",
    "  assert len(translation_results) == len(translation_inputs), \"Test NOT passed: Result should be a list of length equal to the input dataset size.\"\n",
    "  assert min([len(s) for s in translation_results]) > 0, \"Test NOT passed: Translations should be non-empty.\"\n",
    "\n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "def dbTestQuestion1_3(few_shot_pipeline, few_shot_prompt, few_shot_results):\n",
    "  lesson, question = \"lesson1\", \"question3\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert isinstance(few_shot_prompt, str), \"Test NOT passed: Prompt should be a string.\"\n",
    "  assert isinstance(few_shot_results, str), \"Test NOT passed: Results should be a string.\"\n",
    "  assert len(few_shot_prompt) > 0, \"Test NOT passed: Prompt should be non-empty.\"\n",
    "  assert few_shot_results.find(few_shot_prompt) == 0, \"Test NOT passed: Results should be prefixed by the prompt.\"\n",
    "  assert len(few_shot_results) > len(few_shot_prompt), \"Test NOT passed: Results should include new text, beyond the prompt.\"\n",
    "\n",
    "  questionPassed(userhome_for_testing, lesson, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3472ced-8101-454c-bee7-cd3ce47056f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LLM 02L - Embeddings, Vector Databases, and Search\n",
    "\n",
    "def dbTestQuestion2_1(collection_name):\n",
    "  lesson, question = \"lesson2\", \"question1\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert  collection_name==\"my_talks\", \"Test NOT passed: The collection_name should be my_talks.\" \n",
    "\n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "def dbTestQuestion2_2(talks_collection):\n",
    "  lesson, question = \"lesson2\", \"question2\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert  str(type(talks_collection)) == \"<class 'chromadb.api.models.Collection.Collection'>\", \"Test NOT passed: Result should be of type `chromadb.api.models.Collection.Collection`\"\n",
    "\n",
    "  assert talks_collection.count() > 0, \"Test NOT passed: The collection should be non-empty.\"\n",
    "\n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "def dbTestQuestion2_3(results):\n",
    "  lesson, question = \"lesson2\", \"question3\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert  len(results) > 0, \"Test NOT passed: The result must be non-empty, check `query_texts` and `n_results`\"\n",
    "\n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "def dbTestQuestion2_4(pipe):\n",
    "  lesson, question = \"lesson2\", \"question4\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert  str(type(pipe)) == \"<class 'transformers.pipelines.text_generation.TextGenerationPipeline'>\", \"Test NOT passed: Result should be of type `transformers.pipelines.text_generation.TextGenerationPipeline`\"\n",
    "\n",
    "  questionPassed(userhome_for_testing, lesson, question) \n",
    "\n",
    "def dbTestQuestion2_5(_question, context, prompt_template):\n",
    "  # using _question given that `question` is reserved for `questionPassed`\n",
    "  lesson, question = \"lesson2\", \"question5\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert isinstance(_question, str), \"Test NOT passed: `question` should be a `str` type.\"\n",
    "  assert isinstance(context, str), \"Test NOT passed: `context` should be a `str` type.\"\n",
    "  assert _question in prompt_template, \"Test NOT passed: Your `question` should appear inside the prompt.\" \n",
    "  assert context in prompt_template, \"Test NOT passed: Your `context` should appear inside the prompt.\" \n",
    "\n",
    "  questionPassed(userhome_for_testing, lesson, question) \n",
    "\n",
    "def dbTestQuestion2_6(lm_response):\n",
    "  lesson, question = \"lesson2\", \"question6\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert lm_response[0][\"generated_text\"] is not None, \"Test NOT passed:  `lm_response` should not be empty\" \n",
    "\n",
    "  questionPassed(userhome_for_testing, lesson, question) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9013b2f8-bffe-472d-bbda-b25685d29674",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LLM 03L - Building LLM Chains Lab\n",
    "\n",
    "def dbTestQuestion3_1(embeddings, docsearch):\n",
    "  lesson, question = \"lesson3\", \"question1\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert str(type(embeddings)) == \"<class 'langchain.embeddings.huggingface.HuggingFaceEmbeddings'>\", \"Test NOT passed: Result is not of type `langchain.embeddings.huggingface.HuggingFaceEmbeddings`\"\n",
    "  assert str(type(docsearch)) == \"<class 'langchain.vectorstores.chroma.Chroma'>\", \"Test NOT passed: Result is not of type `langchain.vectorstores.chroma.Chroma`\"\n",
    "  \n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "def dbTestQuestion3_2(qa, query_results_hamlet):\n",
    "  lesson, question = \"lesson3\", \"question2\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert str(type(qa)) == \"<class 'langchain.chains.retrieval_qa.base.RetrievalQA'>\", \"Test NOT passed: Result is not of type `langchain.chains.retrieval_qa.base.RetrievalQA`\"\n",
    "  assert type(query_results_hamlet) == str, \"Test NOT passed: Query results not a string\"\n",
    "  \n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "def dbTestQuestion3_3(qa, query_results_venice):\n",
    "  lesson, question = \"lesson3\", \"question3\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert str(type(qa)) == \"<class 'langchain.chains.retrieval_qa.base.RetrievalQA'>\", \"Test NOT passed: Result is not of type `langchain.chains.retrieval_qa.base.RetrievalQA`\"\n",
    "  assert type(query_results_venice) == str, \"Test NOT passed: Query results not a string\"\n",
    "  \n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "def dbTestQuestion3_4(qa, query_results_romeo):\n",
    "  lesson, question = \"lesson3\", \"question4\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert str(type(qa)) == \"<class 'langchain.chains.retrieval_qa.base.RetrievalQA'>\", \"Test NOT passed: Result is not of type `langchain.chains.retrieval_qa.base.RetrievalQA`\"\n",
    "  assert type(query_results_romeo) == str, \"Test NOT passed: Query results not a string\"\n",
    "  \n",
    "  questionPassed(userhome_for_testing, lesson, question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d863dd2a-dec4-492d-8268-cf3b00bf50a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LLM 04L - Fine-tuning LLMs\n",
    "\n",
    "def dbTestQuestion4_1(ds):\n",
    "  lesson, question = \"lesson4\", \"question1\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert str(ds.keys()) == \"dict_keys(['train'])\", \"Test NOT passed: `ds` should be of type `datasets.dataset_dict.DatasetDict`\"\n",
    "  \n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "def dbTestQuestion4_2(model_checkpoint):\n",
    "  lesson, question = \"lesson4\", \"question2\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert  model_checkpoint == \"EleutherAI/pythia-70m-deduped\", \"Test NOT passed: `model_checkpoint` should be `EleutherAI/pythia-70m-deduped`.\"\n",
    "  \n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "def dbTestQuestion4_3(tokenizer):\n",
    "  lesson, question = \"lesson4\", \"question3\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert str(type(tokenizer)) == \"<class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>\", \"Test NOT passed: `tokenizer` is not of type `transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast`\"\n",
    "  \n",
    "  questionPassed(userhome_for_testing, lesson, question) \n",
    "\n",
    "def dbTestQuestion4_4(tokenized_dataset):\n",
    "  lesson, question = \"lesson4\", \"question4\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert str(type(tokenized_dataset)) == \"<class 'datasets.dataset_dict.DatasetDict'>\", \"Test NOT passed: `tokenized_dataset` should be of type `datasets.dataset_dict.DatasetDict`\"\n",
    "  assert  len(tokenized_dataset[\"train\"][\"input_ids\"][0]) == len(tokenized_dataset[\"train\"][\"attention_mask\"][0]), \"Test NOT passed: For each entry the number of `input_ids` and `attention_masks` should be equal\"\n",
    "  \n",
    "  questionPassed(userhome_for_testing, lesson, question) \n",
    "\n",
    "def dbTestQuestion4_5(training_args):\n",
    "  lesson, question = \"lesson4\", \"question5\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert training_args.num_train_epochs == 10, \"Test NOT passed: `num_train_epochs` should be 10.\"\n",
    "  assert str(type(training_args.optim)) == \"<enum 'OptimizerNames'>\", \"Test NOT passed: `optim` should be of type `OptimizerNames`.\"\n",
    "  \n",
    "  questionPassed(userhome_for_testing, lesson, question) \n",
    "\n",
    "def dbTestQuestion4_6(model):\n",
    "  lesson, question = \"lesson4\", \"question6\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert model.base_model_prefix == \"gpt_neox\", \"Test NOT passed: `base_model_prefix should be `gpt_neox`, reload your model checkpoint.\"\n",
    "  \n",
    "  questionPassed(userhome_for_testing, lesson, question) \n",
    "\n",
    "def dbTestQuestion4_7(trainer):\n",
    "  lesson, question = \"lesson4\", \"question7\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert trainer.train_dataset.num_rows == 6000, \"Test NOT passed: The number of rows in the training data is not equal to `TRAINING_SIZE`.\"\n",
    "  \n",
    "  questionPassed(userhome_for_testing, lesson, question) \n",
    "\n",
    "def dbTestQuestion4_8(trainer):\n",
    "  lesson, question = \"lesson4\", \"question8\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert trainer.state.epoch == 10.0, \"Test NOT passed: make sure to run your training for 10 epochs exactly.\"\n",
    "  \n",
    "  questionPassed(userhome_for_testing, lesson, question) \n",
    "\n",
    "def dbTestQuestion4_9(rouge_scores):\n",
    "  lesson, question = \"lesson4\", \"question9\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert type(rouge_scores) == dict, \"Test NOT passed: `rouge_scores should be a dict, check your scoring answer.\"\n",
    "  \n",
    "  questionPassed(userhome_for_testing, lesson, question) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "317fef09-6053-45e8-b8cb-aa7acd73916e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #LLM 05L - LLMs and Society Lab\n",
    "\n",
    "def dbTestQuestion5_1(group1_bold, group2_bold):\n",
    "  lesson, question = \"lesson5\", \"question1\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert  isinstance(group1_bold, list), \"Test NOT passed: `group1_bold` should be of type list.\"\n",
    "  assert  isinstance(group2_bold, list), \"Test NOT passed: `group2_bold` should be of type list.\"\n",
    "\n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "def dbTestQuestion5_2(group1_prompts, group2_prompts):\n",
    "  lesson, question = \"lesson5\", \"question2\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert isinstance(group1_prompts, list), \"Test NOT passed: `group1_prompts` should be of type list.\"\n",
    "  assert isinstance(group2_prompts, list), \"Test NOT passed: `group2_prompts` should be of type list.\"\n",
    "\n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "def dbTestQuestion5_3(group1_continuation, group2_continuation):\n",
    "  lesson, question = \"lesson5\", \"question3\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert isinstance(group1_continuation, list), \"Test NOT passed: `group1_continuation` should be of type list.\"\n",
    "  assert isinstance(group2_continuation, list), \"Test NOT passed: `group2_continuation` should be of type list.\"\n",
    "\n",
    "  questionPassed(userhome_for_testing, lesson, question)\n",
    "\n",
    "def dbTestQuestion5_4(regard_score):\n",
    "  lesson, question = \"lesson5\", \"question4\"\n",
    "  userhome_for_testing = getUsernameFromEnv(lesson)\n",
    "\n",
    "  assert isinstance(regard_score, dict), \"Test NOT passed: The regard score should be of type dictionary.\" \n",
    "\n",
    "  questionPassed(userhome_for_testing, lesson, question) \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Test-Framework",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
