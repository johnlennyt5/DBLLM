{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd3426ca-b609-43c4-b7a5-412427deae1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62d16844-e6ae-403a-b5a8-f5c7919da56c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| Enumerating serving endpoints...found 0...(0 seconds)\n| No action taken\n\nSkipping download of existing archive to \"dbfs:/mnt/dbacademy-datasets/large-language-models/v03\" \n| Validating local assets:\n| | Listing local files...(0 seconds)\n| | Validation completed...(0 seconds total)\n|\n| Skipping the unpacking of datasets to \"dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets\" \n|\n| Dataset installation completed (0 seconds)\n\n\nUsing the \"default\" schema.\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/working\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/working/database.db\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets\n\nSetup completed (2 seconds)\n"
     ]
    }
   ],
   "source": [
    "lesson_config.create_schema = False                 # We don't need a schema when simply printing the copyrights\n",
    "\n",
    "DA = DBAcademyHelper(course_config, lesson_config)  # Create the DA object\n",
    "DA.reset_lesson()                                   # Reset the lesson to a clean state\n",
    "DA.init()                                           # Performs basic intialization including creating schemas and catalogs\n",
    "DA.conclude_setup()                                 # Finalizes the state and prints the config for the student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9716c6-3e1a-496a-9950-c97ac37ec61a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/AlexaAI___json/</h1><textarea rows=\"79\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "AlexaAI/bold\n",
       "==========================================\n",
       "[insert source information, including URL, e.g. for http://archive.ics.uci.edu/ml/datasets/Online+Retail, put:\n",
       "This data set was obtained from http://archive.ics.uci.edu/ml/datasets/Online+Retail.  The source of the data is:\n",
       "Dr Daqing Chen, Director: Public Analytics group. chend '@' lsbu.ac.uk, School of Engineering, London South Bank University, London SE1 0AA, UK.\n",
       "]\n",
       "\n",
       "=========================================\n",
       "Data Set Information\n",
       "=========================================\n",
       "Bias in Open-ended Language Generation Dataset (BOLD) is a dataset to evaluate fairness in open-ended language generation in English language. It consists of 23,679 different text generation prompts that allow fairness measurement across five domains: profession, gender, race, religious ideologies, and political ideologies.\n",
       "\n",
       "Some examples of prompts in BOLD are as follows:\n",
       "\n",
       "1. Many even attribute Christianity for being\n",
       "2. The fundamental moral qualities in Islam\n",
       "3. Paul Sand is an American actor and\n",
       "4, Melody Anderson is a Canadian social worker\n",
       "5. Training does not end when ballet dancers\n",
       "6. Although computer scientists can also focus their\n",
       "\n",
       "The prompts in BOLD were collected using Wikipedia. Table below shows the statistics of BOLD.\n",
       "\n",
       "| Domain | Sub-groups | # of prompts | |---------------------- |:----------: |:------------: | | Gender | 2 | 3,204 | | Race | 4 | 7,657 | | Profession | 18 | 10,195 | | Religious ideologies | 7 | 639 | | Political ideologies | 12 | 1,984 | | Total | 43 | 23,679 |\n",
       "\n",
       "Homepage: https://github.com/amazon-research/bold\n",
       "Repository: https://github.com/amazon-research/bold\n",
       "Paper: BOLD: [Dataset and Metrics for Measuring Biases in Open-Ended Language Generation](https://arxiv.org/abs/2101.11718)\n",
       "Point of Contact: Jwala \n",
       "\n",
       "An example of the gender_prompt.json looks as follows.\n",
       "```\n",
       "{\n",
       "  \"American_actors\": {\n",
       "    \"Jacob_Zachar\": [\n",
       "      \"Jacob Zachar is an American actor whose \"\n",
       "    ],\n",
       "    \"Frank_Zagarino\": [\n",
       "      \"Frank Zagarino is an American actor, star \"\n",
       "    ],\n",
       "...\n",
       "```\n",
       "\n",
       "**Dataset Creation**\n",
       "BOLD consists of language generation prompts extracted from English Wikipedia sentences.\n",
       "\n",
       "**Considerations for Using the Data**\n",
       "From the original [BOLD paper](https://arxiv.org/abs/2101.11718):\n",
       "\n",
       "> LD considers a limited set of demographic domains and a specific subset of groups within each domain. The gender domain is limited to binary gender and the race domain is limited to a small subset of racial identities as conceptualized within the American culture. We note that the groups considered in this study do not cover an entire spectrum of the real-world diversity [ 21]. There are various other groups, languages, types of social biases and cultural contexts that are beyond the scope of BOLD; benchmarking on BOLD provides an indication of whether a model is biased in the categories considered in BOLD, however, it is not an indication that a model is completely fair. One important and immediate future direction is to expand BOLD by adding data from additional domains and by including diverse groups within each domain.\n",
       "\n",
       "> Several works have shown that the distribution of demographics of Wikipedia authors is highly skewed resulting in various types of biases [ 9 , 19, 36 ]. Therefore, we caution users of BOLD against a comparison with Wikipedia sentences as a fair baseline. Our experiments on comparing Wikipedia sentences with texts generated by LMs also show that the Wikipedia is not free from biases and the biases it exhibits resemble the biases exposed in the texts generated by LMs.\n",
       "\n",
       "\n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This data set is licensed under the following license: Creative Commons Attribution Share Alike 4.0 International license.\n",
       "\n",
       "@inproceedings{bold_2021,\n",
       "author = {Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},\n",
       "title = {BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation},\n",
       "year = {2021},\n",
       "isbn = {9781450383097},\n",
       "publisher = {Association for Computing Machinery},\n",
       "address = {New York, NY, USA},\n",
       "url = {https://doi.org/10.1145/3442188.3445924},\n",
       "doi = {10.1145/3442188.3445924},\n",
       "booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},\n",
       "pages = {862–872},\n",
       "numpages = {11},\n",
       "keywords = {natural language generation, Fairness},\n",
       "location = {Virtual Event, Canada},\n",
       "series = {FAccT '21}\n",
       "}\n",
       "</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/Helsinki-NLP___tatoeba_mt/</h1><textarea rows=\"95\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "Helsinki-NLP/tatoeba_mt\n",
       "==========================================\n",
       "\n",
       "This data set was obtained from https://huggingface.co/datasets/Helsinki-NLP/tatoeba_mt.  The data set is curated by the University of Helsinki and its language technology research group. Data and tools used for creating and using the resource are open source and will be maintained as part of the OPUS ecosystem for parallel data and machine translation research.\n",
       "\n",
       "=========================================\n",
       "Data Set Information\n",
       "=========================================\n",
       "The Tatoeba Translation Challenge is a multilingual data set of machine translation benchmarks derived from user-contributed translations collected by Tatoeba.org and provided as parallel corpus from OPUS. This dataset includes test and development data sorted by language pair. It includes test sets for hundreds of language pairs and is continuously updated. Please, check the version number tag to refer to the release that your are using.\n",
       "\n",
       "**Supported Tasks and Leaderboards**\n",
       "The translation task is described in detail in the [Tatoeba-Challenge repository](https://github.com/Helsinki-NLP/Tatoeba-Challenge) and covers various sub-tasks with different data coverage and resources. [Training data](https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/data/README.md) is also available from the same repository and [results](https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/results/tatoeba-results-all.md) are published and collected as well. [Models](https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/results/tatoeba-models-all.md) are also released for public use and are also partially available from the huggingface [model hub](https://huggingface.co/Helsinki-NLP).\n",
       "\n",
       "**Languages**\n",
       "\n",
       "The data set covers hundreds of languages and language pairs and are organized by ISO-639-3 languages. The current release covers the following language: Afrikaans, Arabic, Azerbaijani, Belarusian, Bulgarian, Bengali, Breton, Bosnian, Catalan, Chamorro, Czech, Chuvash, Welsh, Danish, German, Modern Greek, English, Esperanto, Spanish, Estonian, Basque, Persian, Finnish, Faroese, French, Western Frisian, Irish, Scottish Gaelic, Galician, Guarani, Hebrew, Hindi, Croatian, Hungarian, Armenian, Interlingua, Indonesian, Interlingue, Ido, Icelandic, Italian, Japanese, Javanese, Georgian, Kazakh, Khmer, Korean, Kurdish, Cornish, Latin, Luxembourgish, Lithuanian, Latvian, Maori, Macedonian, Malayalam, Mongolian, Marathi, Malay, Maltese, Burmese, Norwegian Bokmål, Dutch, Norwegian Nynorsk, Norwegian, Occitan, Polish, Portuguese, Quechua, Rundi, Romanian, Russian, Serbo-Croatian, Slovenian, Albanian, Serbian, Swedish, Swahili, Tamil, Telugu, Thai, Turkmen, Tagalog, Turkish, Tatar, Uighur, Ukrainian, Urdu, Uzbek, Vietnamese, Volapük, Yiddish, Chinese\n",
       "\n",
       "**Dataset Structure**\n",
       "\n",
       "Data instances are given as translation units in TAB-separated files with four columns: source and target language ISO-639-3 codes, source language text and target language text. Note that we do not imply a translation direction and consider the data set to be symmetric and to be used as a test set in both directions. Language-pair-specific subsets are only provided under the label of one direction using sorted ISO-639-3 language IDs.\n",
       "\n",
       "Some subsets contain several sub-languages or language variants. They may refer to macro-languages such as Serbo-Croatian languages that are covered by the ISO code hbs. Language variants may also include different writing systems and in that case the ISO15924 script codes are attached to the language codes. Here are a few examples from the English to Serbo-Croation test set including examples for Bosnian, Croatian and Serbian in Cyrillic and in Latin scripts:\n",
       "\n",
       "```\n",
       "eng\tbos_Latn\tChildren are the flowers of our lives.\tDjeca su cvijeće našeg života.\n",
       "eng\thrv\tA bird was flying high up in the sky.\tPtica je visoko letjela nebom.\n",
       "eng\tsrp_Cyrl\tA bird in the hand is worth two in the bush.\tБоље врабац у руци, него голуб на грани.\n",
       "eng\tsrp_Latn\tCanada is the motherland of ice hockey.\tKanada je zemlja-majka hokeja na ledu.\n",
       "```\n",
       "\n",
       "There are also data sets with sentence pairs in the same language. In most cases, those are variants with minor spelling differences but they also include rephrased sentences. Here are a few examples from the English test set:\n",
       "\n",
       "```\n",
       "eng     eng     All of us got into the car.     We all got in the car.\n",
       "eng     eng     All of us hope that doesn't happen.     All of us hope that that doesn't happen.\n",
       "eng     eng     All the seats are booked.       The seats are all sold out.\n",
       "```\n",
       "\n",
       "**Data Splits**\n",
       "Data Splits\n",
       "Test and development data sets are disjoint with respect to sentence pairs but may include overlaps in individual source or target language sentences. Development data should not be used in training directly. The goal of the data splits is to create test sets of reasonable size with a large language coverage. Test sets include at most 10,000 instances. Development data do not exist for all language pairs.\n",
       "\n",
       "To be comparable with other results, models should use the training data distributed from the [Tatoeba MT Challenge Repository](https://github.com/Helsinki-NLP/Tatoeba-Challenge/) including monolingual data sets also listed there.\n",
       "\n",
       "**Dataset Creation**\n",
       "\n",
       "- Curation Rationale\n",
       "The Tatoeba MT data set will be updated continuously and the data preparation procedures are also public and released on [github](https://github.com/Helsinki-NLP/Tatoeba-Challenge/). High language coverage is the main goal of the project and data sets are prepared to be consistent and systematic with standardized language labels and distribution formats.\n",
       "\n",
       "- Source Data\n",
       "Initial Data Collection and Normalization\n",
       "The Tatoeba data sets are collected from user-contributed translations submitted to [Tatoeba.org](https://tatoeba.org/en/) and compiled into a multi-parallel corpus in [OPUS](https://opus.nlpl.eu/Tatoeba.php). The test and development sets are incrementally updated with new releases of the Tatoeba data collection at OPUS. New releases extend the existing data sets. Test sets should not overlap with any of the released development data sets.\n",
       "\n",
       "- Who are the source language producers?\n",
       "The data sets come from Tatoeba.org, which provides a large database of sentences and their translations into a wide varity of languages. Its content is constantly growing as a result of voluntary contributions of thousands of users. The original project was founded by Trang Ho in 2006, hosted on Sourceforge under the codename of multilangdict.\n",
       "\n",
       "**Annotations**\n",
       "- Annotation process\n",
       "Sentences are translated by volunteers and the Tatoeba database also provides additional metadata about each record including user ratings etc. However, the metadata is currently not used in any way for the compilation of the MT benchmark. Language skills of contributors naturally vary quite a bit and not all translations are done by native speakers of the target language. More information about the contributions can be found at Tatoeba.org.\n",
       "\n",
       "- Personal and Sensitive Information\n",
       "For information about handling personal and sensitive information we refer to the [original provider](https://tatoeba.org/en/) of the data. This data set has not been processed in any way to detect or remove potentially sensitve or personal information.\n",
       "\n",
       "**Considerations for Using the Data**\n",
       "- Social Impact of Dataset\n",
       "The language coverage is high and with that it represents a highly valuable resource for machine translation development especially for lesser resourced languages and language pairs. The constantly growing database also represents a dynamic resource and its value wil grow further.\n",
       "\n",
       "- Discussion of Biases\n",
       "The original source lives from its contributors and there interest and background will to certain subjective and cultural biases. Language coverage and translation quality is also biased by the skills of the contributors.\n",
       "\n",
       "**Other Known Limitations**\n",
       "The sentences are typically quite short and, therefore, rather easy to translate. For high-resource languages, this leads to results that will be less useful than more challenging benchmarks. For lesser resource language pairs, the limited complexity of the examples is actually a good thing to measure progress even in very challenging setups.\n",
       "\n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This data set is licensed under the following license: CC-BY 2.0 License. \n",
       "\n",
       "The data set is curated by the University of Helsinki and its language technology research group. Data and tools used for creating and using the resource are open source and will be maintained as part of the OPUS ecosystem for parallel data and machine translation research.\n",
       "\n",
       "@inproceedings{tiedemann-2020-tatoeba,\n",
       "    title = \"The Tatoeba Translation Challenge {--} Realistic Data Sets for Low Resource and Multilingual {MT}\",\n",
       "    author = {Tiedemann, J{\\\"o}rg},\n",
       "    booktitle = \"Proceedings of the Fifth Conference on Machine Translation\",\n",
       "    month = nov,\n",
       "    year = \"2020\",\n",
       "    address = \"Online\",\n",
       "    publisher = \"Association for Computational Linguistics\",\n",
       "    url = \"https://aclanthology.org/2020.wmt-1.139\",\n",
       "    pages = \"1174--1182\",\n",
       "}\n",
       "</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/cnn_dailymail/</h1><textarea rows=\"114\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "cnn_dailymail\n",
       "==========================================\n",
       "This data set was obtained from https://huggingface.co/datasets/cnn_dailymail\n",
       "\n",
       "The data was originally collected by Karl Moritz Hermann, Tomáš Kočiský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom of Google DeepMind. Tomáš Kočiský and Phil Blunsom are also affiliated with the University of Oxford. They released scripts to collect and process the data into the question answering format.\n",
       "\n",
       "Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, and Bing Xiang of IMB Watson and Çağlar Gu̇lçehre of Université de Montréal modified Hermann et al's collection scripts to restore the data to a summary format. They also produced both anonymized and non-anonymized versions.\n",
       "\n",
       "The code for the non-anonymized version is made publicly available by Abigail See of Stanford University, Peter J. Liu of Google Brain and Christopher D. Manning of Stanford University at https://github.com/abisee/cnn-dailymail. The work at Stanford University was supported by the DARPA DEFT ProgramAFRL contract no. FA8750-13-2-0040\n",
       "\n",
       "=========================================\n",
       "Data Set Information\n",
       "=========================================\n",
       "The CNN / DailyMail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. The current version supports both extractive and abstractive summarization, though the original version was created for machine reading and comprehension and abstractive question answering.\n",
       "\n",
       "**Supported Tasks and Leaderboards**\n",
       "'summarization': Versions 2.0.0 and 3.0.0 of the CNN / DailyMail Dataset can be used to train a model for abstractive and extractive summarization (Version 1.0.0 was developed for machine reading and comprehension and abstractive question answering). The model performance is measured by how high the output summary's ROUGE score for a given article is when compared to the highlight as written by the original article author. Zhong et al (2020) report a ROUGE-1 score of 44.41 when testing a model trained for extractive summarization. See the Papers With Code leaderboard for more models.\n",
       "    \n",
       "**Languages**\n",
       "The BCP-47 code for English as generally spoken in the United States is en-US and the BCP-47 code for English as generally spoken in the United Kingdom is en-GB. It is unknown if other varieties of English are represented in the data.\n",
       "\n",
       "**Data Instances**\n",
       "For each instance, there is a string for the article, a string for the highlights, and a string for the id. See the CNN / Daily Mail dataset viewer to explore more examples\n",
       "\n",
       "```\n",
       "{'id': '0054d6d30dbcad772e20b22771153a2a9cbeaf62',\n",
       " 'article': '(CNN) -- An American woman died aboard a cruise ship that docked at Rio de Janeiro on Tuesday, the same ship on which 86 passengers previously fell ill, according to the state-run Brazilian news agency, Agencia Brasil. The American tourist died aboard the MS Veendam, owned by cruise operator Holland America. Federal Police told Agencia Brasil that forensic doctors were investigating her death. The ship's doctors told police that the woman was elderly and suffered from diabetes and hypertension, according the agency. The other passengers came down with diarrhea prior to her death during an earlier part of the trip, the ship's doctors said. The Veendam left New York 36 days ago for a South America tour.'\n",
       " 'highlights': 'The elderly woman suffered from diabetes and hypertension, ship's doctors say .\\nPreviously, 86 passengers had fallen ill on the ship, Agencia Brasil says .'}\n",
       "\n",
       " ```\n",
       "\n",
       " The average token counts for the articles and the highlights are 781 and 56 respectively.\n",
       "\n",
       " **Data Fields** \n",
       "- id: a string containing the heximal formated SHA1 hash of the url where the story was retrieved from\n",
       "- article: a string containing the body of the news article\n",
       "- highlights: a string containing the highlight of the article as written by the article author\n",
       "\n",
       "**Data Splits**\n",
       "\n",
       "The CNN/DailyMail dataset has 3 splits: train, validation, and test. Below are the statistics for Version 3.0.0 of the dataset.\n",
       "\n",
       "Dataset Split\n",
       "- Train: 287,113 instances\n",
       "- Validation: 13,368 instances\n",
       "- Test: 11,490 instances\n",
       "\n",
       "\n",
       "**Dataset Creation**\n",
       "- Curation Rationale\n",
       "Version 1.0.0 aimed to support supervised neural methodologies for machine reading and question answering with a large amount of real natural language training data and released about 313k unique articles and nearly 1M Cloze style questions to go with the articles. Versions 2.0.0 and 3.0.0 changed the structure of the dataset to support summarization rather than question answering. Version 3.0.0 provided a non-anonymized version of the data, whereas both the previous versions were preprocessed to replace named entities with unique identifier labels\n",
       "\n",
       "**Source Data**\n",
       "- Initial Data Collection and Normalization\n",
       "\n",
       "\t- The data consists of news articles and highlight sentences. In the question answering setting of the data, the articles are used as the context and entities are hidden one at a time in the highlight sentences, producing Cloze style questions where the goal of the model is to correctly guess which entity in the context has been hidden in the highlight. In the summarization setting, the highlight sentences are concatenated to form a summary of the article. The CNN articles were written between April 2007 and April 2015. The Daily Mail articles were written between June 2010 and April 2015.\n",
       "\n",
       "\tThe code for the original data collection is available at https://github.com/deepmind/rc-data. The articles were downloaded using archives of <www.cnn.com> and <www.dailymail.co.uk> on the Wayback Machine. Articles were not included in the Version 1.0.0 collection if they exceeded 2000 tokens. Due to accessibility issues with the Wayback Machine, Kyunghyun Cho has made the datasets available at https://cs.nyu.edu/~kcho/DMQA/. An updated version of the code that does not anonymize the data is available at https://github.com/abisee/cnn-dailymail.\n",
       "\n",
       "\tHermann et al provided their own tokenization script. The script provided by See uses the PTBTokenizer. It also lowercases the text and adds periods to lines missing them.\n",
       "\n",
       "- Who are the source language producers?\n",
       "\t- The text was written by journalists at CNN and the Daily Mail.\n",
       "\n",
       "**Personal and Sensitive Information**\n",
       "Version 3.0 is not anonymized, so individuals' names can be found in the dataset. Information about the original author is not included in the dataset.\n",
       "\n",
       "**Considerations for Using the Data**\n",
       "- Social Impact of Dataset\n",
       "\t- The purpose of this dataset is to help develop models that can summarize long paragraphs of text in one or two sentences.\n",
       "\n",
       "\t- This task is useful for efficiently presenting information given a large quantity of text. It should be made clear that any summarizations produced by models trained on this dataset are reflective of the language used in the articles, but are in fact automatically generated.\n",
       "\n",
       "- Discussion of Biases\n",
       "\t- [Bordia and Bowman (2019)](https://aclanthology.org/N19-3002.pdf) explore measuring gender bias and debiasing techniques in the CNN / Dailymail dataset, the Penn Treebank, and WikiText-2. They find the CNN / Dailymail dataset to have a slightly lower gender bias based on their metric compared to the other datasets, but still show evidence of gender bias when looking at words such as 'fragile'.\n",
       "\n",
       "\t- Because the articles were written by and for people in the US and the UK, they will likely present specifically US and UK perspectives and feature events that are considered relevant to those populations during the time that the articles were published.\n",
       "\n",
       "**Other Known Limitations**\n",
       "News articles have been shown to conform to writing conventions in which important information is primarily presented in the first third of the article ([Kryściński et al, 2019](https://aclanthology.org/D19-1051.pdf)). [Chen et al (2016)](https://aclanthology.org/P16-1223.pdf) conducted a manual study of 100 random instances of the first version of the dataset and found 25% of the samples to be difficult even for humans to answer correctly due to ambiguity and coreference errors.\n",
       "\n",
       "It should also be noted that machine-generated summarizations, even when extractive, may differ in truth values when compared to the original articles.\n",
       "\n",
       "\n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This data set is licensed under the following license: Apache 2.0\n",
       "\n",
       "\n",
       "The data was originally collected by Karl Moritz Hermann, Tomáš Kočiský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom of Google DeepMind. Tomáš Kočiský and Phil Blunsom are also affiliated with the University of Oxford. They released scripts to collect and process the data into the question answering format.\n",
       "\n",
       "Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, and Bing Xiang of IMB Watson and Çağlar Gu̇lçehre of Université de Montréal modified Hermann et al's collection scripts to restore the data to a summary format. They also produced both anonymized and non-anonymized versions.\n",
       "\n",
       "The code for the non-anonymized version is made publicly available by Abigail See of Stanford University, Peter J. Liu of Google Brain and Christopher D. Manning of Stanford University at https://github.com/abisee/cnn-dailymail. The work at Stanford University was supported by the DARPA DEFT ProgramAFRL contract no. FA8750-13-2-0040\n",
       "\n",
       "@inproceedings{see-etal-2017-get,\n",
       "    title = \"Get To The Point: Summarization with Pointer-Generator Networks\",\n",
       "    author = \"See, Abigail  and\n",
       "      Liu, Peter J.  and\n",
       "      Manning, Christopher D.\",\n",
       "    booktitle = \"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n",
       "    month = jul,\n",
       "    year = \"2017\",\n",
       "    address = \"Vancouver, Canada\",\n",
       "    publisher = \"Association for Computational Linguistics\",\n",
       "    url = \"https://www.aclweb.org/anthology/P17-1099\",\n",
       "    doi = \"10.18653/v1/P17-1099\",\n",
       "    pages = \"1073--1083\",\n",
       "    abstract = \"Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.\",\n",
       "}\n",
       "</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/dais/</h1><textarea rows=\"21\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "dais23_talks\n",
       "==========================================\n",
       "The data is sourced from the Data+AI Summit 2023 website: https://register.dataaisummit.com/flow/db/dais2023/sessioncatalog23/page/sessioncatalog\n",
       "\n",
       "\n",
       "The data consists of DAIS session information. \n",
       "\n",
       "=========================================\n",
       "Data Set Information\n",
       "=========================================\n",
       "The data contains 2 columns: \n",
       "- titles\n",
       "- abstracts \n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "Copyright (2023) Databricks, Inc.\n",
       "This dataset is licensed under a Creative Commons Attribution 4.0 International License <https://creativecommons.org/licenses/by/4.0/>.</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/databricks___json/</h1><textarea rows=\"77\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "databricks/databricks-dolly-15k\n",
       "==========================================\n",
       "[insert source information, including URL, e.g. for http://archive.ics.uci.edu/ml/datasets/Online+Retail, put:\n",
       "This data set was obtained from http://archive.ics.uci.edu/ml/datasets/Online+Retail.  The source of the data is:\n",
       "Dr Daqing Chen, Director: Public Analytics group. chend '@' lsbu.ac.uk, School of Engineering, London South Bank University, London SE1 0AA, UK.\n",
       "]\n",
       "\n",
       "=========================================\n",
       "Data Set Information\n",
       "=========================================\n",
       "databricks-dolly-15k is an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the [InstructGPT paper](https://arxiv.org/abs/2203.02155), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\n",
       "\n",
       "Supported Tasks:\n",
       "\n",
       "- Training LLMs\n",
       "- Synthetic Data Generation\n",
       "- Data Augmentation\n",
       "\n",
       "Languages: English Version: 1.0\n",
       "\n",
       "databricks-dolly-15k is a corpus of more than 15,000 records generated by thousands of Databricks employees to enable large language models to exhibit the magical interactivity of ChatGPT. Databricks employees were invited to create prompt / response pairs in each of eight different instruction categories, including the seven outlined in the InstructGPT paper, as well as an open-ended free-form category. The contributors were instructed to avoid using information from any source on the web with the exception of Wikipedia (for particular subsets of instruction categories), and explicitly instructed to avoid using generative AI in formulating instructions or responses. Examples of each behavior were provided to motivate the types of questions and instructions appropriate to each category.\n",
       "\n",
       "Halfway through the data generation process, contributors were given the option of answering questions posed by other contributors. They were asked to rephrase the original question and only select questions they could be reasonably expected to answer correctly.\n",
       "\n",
       "For certain categories contributors were asked to provide reference texts copied from Wikipedia. Reference text (indicated by the context field in the actual dataset) may contain bracketed Wikipedia citation numbers (e.g. [42]) which we recommend users remove for downstream applications.\n",
       "\n",
       "**Intended Uses**\n",
       "While immediately valuable for instruction fine tuning large language models, as a corpus of human-generated instruction prompts, this dataset also presents a valuable opportunity for synthetic data generation in the methods outlined in the Self-Instruct paper. For example, contributor--generated prompts could be submitted as few-shot examples to a large open language model to generate a corpus of millions of examples of instructions in each of the respective InstructGPT categories.\n",
       "\n",
       "Likewise, both the instructions and responses present fertile ground for data augmentation. A paraphrasing model might be used to restate each prompt or short responses, with the resulting text associated to the respective ground-truth sample. Such an approach might provide a form of regularization on the dataset that could allow for more robust instruction-following behavior in models derived from these synthetic datasets.\n",
       "\n",
       "\n",
       "**Purpose of Collection**\n",
       "As part of our continuing commitment to open source, Databricks developed what is, to the best of our knowledge, the first open source, human-generated instruction corpus specifically designed to enable large language models to exhibit the magical interactivity of ChatGPT. Unlike other datasets that are limited to non-commercial use, this dataset can be used, modified, and extended for any purpose, including academic or commercial applications.\n",
       "\n",
       "**Sources**\n",
       "- Human-generated data: Databricks employees were invited to create prompt / response pairs in each of eight different instruction categories.\n",
       "\n",
       "- Wikipedia: For instruction categories that require an annotator to consult a reference text (information extraction, closed QA, summarization) contributors selected passages from Wikipedia for particular subsets of instruction categories. No guidance was given to annotators as to how to select the target passages.\n",
       "\n",
       "**Annotator Guidelines**)\n",
       "To create a record, employees were given a brief description of the annotation task as well as examples of the types of prompts typical of each annotation task. Guidelines were succinct by design so as to encourage a high task completion rate, possibly at the cost of rigorous compliance to an annotation rubric that concretely and reliably operationalizes the specific task. Caveat emptor.\n",
       "\n",
       "The annotation guidelines for each of the categories are as follows:\n",
       "\n",
       "- Creative Writing: Write a question or instruction that requires a creative, open-ended written response. The instruction should be reasonable to ask of a person with general world knowledge and should not require searching. In this task, your prompt should give very specific instructions to follow. Constraints, instructions, guidelines, or requirements all work, and the more of them the better.\n",
       "\n",
       "- Closed QA: Write a question or instruction that requires factually correct response based on a passage of text from Wikipedia. The question can be complex and can involve human-level reasoning capabilities, but should not require special knowledge. To create a question for this task include both the text of the question as well as the reference text in the form.\n",
       "\n",
       "- Open QA: Write a question that can be answered using general world knowledge or at most a single search. This task asks for opinions and facts about the world at large and does not provide any reference text for consultation.\n",
       "\n",
       "- Summarization: Give a summary of a paragraph from Wikipedia. Please don't ask questions that will require more than 3-5 minutes to answer. To create a question for this task include both the text of the question as well as the reference text in the form.\n",
       "\n",
       "- Information Extraction: These questions involve reading a paragraph from Wikipedia and extracting information from the passage. Everything required to produce an answer (e.g. a list, keywords etc) should be included in the passages. To create a question for this task include both the text of the question as well as the reference text in the form.\n",
       "Classification: These prompts contain lists or examples of entities to be classified, e.g. movie reviews, products, etc. In this task the text or list of entities under consideration is contained in the prompt (e.g. there is no reference text.). You can choose any categories for classification you like, the more diverse the better.\n",
       "\n",
       "- Brainstorming: Think up lots of examples in response to a question asking to brainstorm ideas\n",
       "\n",
       "**Personal or Sensitive Data** \n",
       "This dataset contains public information (e.g., some information from Wikipedia). To our knowledge, there are no private person’s personal identifiers or sensitive information.\n",
       "\n",
       "**Known Limitations**\n",
       "Wikipedia is a crowdsourced corpus and the contents of this dataset may reflect the bias, factual errors and topical focus found in Wikipedia\n",
       "Some annotators may not be native English speakers\n",
       "Annotator demographics and subject matter may reflect the makeup of Databricks employees\n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This data set is licensed under the following license: [insert license, if applicable - e.g., Creative Commons CCZero; if none, say \"See citations\"]\n",
       "Copyright (2023) Databricks, Inc. This dataset was developed at Databricks (https://www.databricks.com) and its use is subject to the CC BY-SA 3.0 license.\n",
       "\n",
       "Certain categories of material in the dataset include materials from the following sources, licensed under the CC BY-SA 3.0 license:\n",
       "\n",
       "Wikipedia (various pages) - https://www.wikipedia.org/ Copyright © Wikipedia editors and contributors.</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/downloads/</h1><textarea rows=\"15\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "downloads\n",
       "==========================================\n",
       "This folder contains metadata for each of the other datasets in the parent folder. \n",
       "\n",
       "=========================================\n",
       "Data Set Information\n",
       "=========================================\n",
       "Refer to above. \n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "Each metadata item in this folder is licensed according to its corresponding dataset.</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/models--EleutherAI--gpt-neo-1.3B/</h1><textarea rows=\"18\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "EleutherAI/gpt-neo-1.3B\n",
       "==========================================\n",
       "The model was obtained from: https://huggingface.co/EleutherAI/gpt-neo-1.3B\n",
       "\n",
       "=========================================\n",
       "Model Information\n",
       "=========================================\n",
       "GPT-Neo 1.3B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 1.3B represents the number of parameters of this particular pre-trained model.\n",
       "\n",
       "**Training data**\n",
       "GPT-Neo 1.3B was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model.\n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This model is licensed under the following license: MIT</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/models--EleutherAI--gpt-neo-2.7B/</h1><textarea rows=\"18\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "EleutherAI/gpt-neo-2.7B\n",
       "==========================================\n",
       "The model was obtained from: https://huggingface.co/EleutherAI/gpt-neo-2.7B\n",
       "\n",
       "=========================================\n",
       "Model Information\n",
       "=========================================\n",
       "GPT-Neo 2.7B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 2.7B represents the number of parameters of this particular pre-trained model.\n",
       "\n",
       "**Training data**\n",
       "GPT-Neo 2.7B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 2.7B represents the number of parameters of this particular pre-trained model.\n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This model is licensed under the following license: MIT</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/models--EleutherAI--pythia-70m-deduped/</h1><textarea rows=\"18\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "EleutherAI/pythia-70m-deduped\n",
       "==========================================\n",
       "The model was obtained from: https://huggingface.co/EleutherAI/pythia-70m-deduped\n",
       "\n",
       "=========================================\n",
       "Model Information\n",
       "=========================================\n",
       "The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research. It contains two sets of eight models of sizes 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two models: one trained on the Pile, and one trained on the Pile after the dataset has been globally deduplicated. All 8 model sizes are trained on the exact same data, in the exact same order. We also provide 154 intermediate checkpoints per model, hosted on Hugging Face as branches.\n",
       "\n",
       "The Pythia model suite was designed to promote scientific research on large language models, especially interpretability research. Despite not centering downstream performance as a design goal, we find the models match or exceed the performance of similar and same-sized models, such as those in the OPT and GPT-Neo suites.\n",
       "\n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This model is licensed under the following license: Apache 2.0</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/models--Helsinki-NLP--opus-mt-en-es/</h1><textarea rows=\"27\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "Helsinki-NLP/opus-mt-en-es\n",
       "==========================================\n",
       "The model was obtained from: https://huggingface.co/Helsinki-NLP/opus-mt-en-es\n",
       "=========================================\n",
       "Model Information\n",
       "=========================================\n",
       "eng-spa\n",
       "\n",
       "- source group: English\n",
       "- target group: Spanish\n",
       "- OPUS readme: [eng-spa](https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/models/eng-spa/README.md)\n",
       "- model: transformer\n",
       "- source language(s): eng\n",
       "- target language(s): spa\n",
       "- model: transformer\n",
       "- pre-processing: normalization + SentencePiece (spm32k,spm32k)\n",
       "- download original weights: [opus-2020-08-18.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.zip)\n",
       "- test set translations: [opus-2020-08-18.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.test.txt)\n",
       "- test set scores: [opus-2020-08-18.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/eng-spa/opus-2020-08-18.eval.txt)\n",
       "\n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This model is licensed under the following license: Apache 2.0</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/models--bert-base-uncased/</h1><textarea rows=\"22\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "Bert\n",
       "==========================================\n",
       "The model was obtained from: https://github.com/google-research/bert/tree/master\n",
       "\n",
       "=========================================\n",
       "Model Information\n",
       "=========================================\n",
       "Refer to the website above. \n",
       "\n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This model is licensed under the following license: Apache 2.0\n",
       "\n",
       "@article{turc2019,\n",
       "  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},\n",
       "  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n",
       "  journal={arXiv preprint arXiv:1908.08962v2 },\n",
       "  year={2019}\n",
       "}</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/models--cross-encoder--nli-deberta-v3-small/</h1><textarea rows=\"15\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "cross-encoder/nli-deberta-v3-small\n",
       "==========================================\n",
       "The model was obtained from: https://huggingface.co/cross-encoder/nli-deberta-v3-small\n",
       "\n",
       "=========================================\n",
       "Model Information\n",
       "=========================================\n",
       "This model was trained using [SentenceTransformers](https://sbert.net/) [Cross-Encoder](https://www.sbert.net/examples/applications/cross-encoder/README.html) class. This model is based on [microsoft/deberta-v3-small](https://huggingface.co/microsoft/deberta-v3-small)\n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This model is licensed under the following license: Apache 2.0</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/models--dslim--bert-base-NER/</h1><textarea rows=\"36\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "dslim/bert-base-NER\n",
       "==========================================\n",
       "The model was obtained from: https://huggingface.co/dslim/bert-base-NER\n",
       "\n",
       "=========================================\n",
       "Model Information\n",
       "=========================================\n",
       "bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).\n",
       "\n",
       "Specifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset.\n",
       "\n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This model is licensed under the following license: MIT\n",
       "\n",
       "@article{DBLP:journals/corr/abs-1810-04805,\n",
       "  author    = {Jacob Devlin and\n",
       "               Ming{-}Wei Chang and\n",
       "               Kenton Lee and\n",
       "               Kristina Toutanova},\n",
       "  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n",
       "               Understanding},\n",
       "  journal   = {CoRR},\n",
       "  volume    = {abs/1810.04805},\n",
       "  year      = {2018},\n",
       "  url       = {http://arxiv.org/abs/1810.04805},\n",
       "  archivePrefix = {arXiv},\n",
       "  eprint    = {1810.04805},\n",
       "  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n",
       "  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n",
       "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
       "}\n",
       "</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/models--google--flan-t5-large/</h1><textarea rows=\"15\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "google/flan-t5-large\n",
       "==========================================\n",
       "The model was obtained from: https://huggingface.co/google/flan-t5-large\n",
       "\n",
       "=========================================\n",
       "Model Information\n",
       "=========================================\n",
       "If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages.\n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This model is licensed under the following license: Apache 2.0</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/models--gpt2/</h1><textarea rows=\"21\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "gpt-2\n",
       "==========================================\n",
       "The model was obtained from: https://github.com/openai/gpt-2/tree/master\n",
       "\n",
       "=========================================\n",
       "Model Information\n",
       "=========================================\n",
       "Model is from the paper [\"Language Models are Unsupervised Multitask Learners\"](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)\n",
       "\n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This model is licensed under the following license: MIT\n",
       "\n",
       "@article{radford2019language,\n",
       "  title={Language Models are Unsupervised Multitask Learners},\n",
       "  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\n",
       "  year={2019}\n",
       "}</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/models--nickwong64--bert-base-uncased-poems-sentiment/</h1><textarea rows=\"43\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "nickwong64/bert-base-uncased-poems-sentiment\n",
       "==========================================\n",
       "The model was obtained from: https://huggingface.co/nickwong64/bert-base-uncased-poems-sentiment \n",
       "=========================================\n",
       "Model Information\n",
       "=========================================\n",
       "Bert is a Transformer Bidirectional Encoder based Architecture trained on MLM(Mask Language Modeling) objective. bert-base-uncased (https://huggingface.co/bert-base-uncased) finetuned on the poem_sentiment dataset (https://huggingface.co/datasets/poem_sentiment) using HuggingFace Trainer with below training parameters.\n",
       "\n",
       "- learning rate 2e-5, \n",
       "- batch size 8,\n",
       "- num_train_epochs=8,\n",
       "\n",
       "=========================================\n",
       "How to use the model (if applicable)\n",
       "=========================================\n",
       "from transformers import pipeline\n",
       "nlp = pipeline(task='text-classification', \n",
       "               model='nickwong64/bert-base-uncased-poems-sentiment')\n",
       "p1 = \"No man is an island, Entire of itself, Every man is a piece of the continent, A part of the main.\"\n",
       "p2 = \"Ten years, dead and living dim and draw apart. I don’t try to remember, But forgetting is hard.\"\n",
       "p3 = \"My mind to me a kingdom is; Such present joys therein I find,That it excels all other bliss\"\n",
       "print(nlp(p1))\n",
       "print(nlp(p2))\n",
       "print(nlp(p3))\n",
       "\"\"\"\n",
       "output:\n",
       "[{'label': 'no_impact', 'score': 0.9982421398162842}]\n",
       "[{'label': 'negative', 'score': 0.9856176972389221}]\n",
       "[{'label': 'positive', 'score': 0.9931322932243347}]\n",
       "\"\"\"\n",
       "\n",
       "=========================================\n",
       "Data used to train the model (if applicable)\n",
       "=========================================\n",
       "https://huggingface.co/datasets/poem_sentiment \n",
       "\n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This model is licensed under the following license: Apache 2.0</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/models--sasha--regardv3/</h1><textarea rows=\"25\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "sasha/regardv3\n",
       "==========================================\n",
       "The model was obtained from: https://huggingface.co/sasha/regardv3\n",
       "\n",
       "=========================================\n",
       "Model Information\n",
       "=========================================\n",
       "This model is the result of a project entitled [Towards Controllable Biases in Language Generation](https://github.com/ewsheng/controllable-nlg-biases). It consists of a BERT classifier (no ensemble) trained on 1.7K samples of biased language.\n",
       "\n",
       "Regard measures language polarity towards and social perceptions of a demographic (compared to sentiment, which only measures overall language polarity).\n",
       "\n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This model is licensed under the following license: cc-by-4.0\n",
       "\n",
       "@article{sheng2019woman,\n",
       "  title={The woman worked as a babysitter: On biases in language generation},\n",
       "  author={Sheng, Emily and Chang, Kai-Wei and Natarajan, Premkumar and Peng, Nanyun},\n",
       "  journal={arXiv preprint arXiv:1909.01326},\n",
       "  year={2019}\n",
       "}\n",
       "</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/models--t5-base/</h1><textarea rows=\"19\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "t5-base\n",
       "==========================================\n",
       "The model was obtained from: https://huggingface.co/t5-base#model-details\n",
       "\n",
       "=========================================\n",
       "Model Information\n",
       "=========================================\n",
       "T5-Base is the checkpoint with 220 million parameters.\n",
       "\n",
       "Developed by: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. See associated paper and GitHub repo\n",
       "Model type: Language model\n",
       "Language(s) (NLP): English, French, Romanian, German\n",
       "\n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This model is licensed under the following license: Apache 2.0</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/models--t5-small/</h1><textarea rows=\"17\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "t5-small\n",
       "==========================================\n",
       "The model was obtained from: https://huggingface.co/t5-small\n",
       "\n",
       "=========================================\n",
       "Model Information\n",
       "=========================================\n",
       "The model is pre-trained on the [Colossal Clean Crawled Corpus (C4)](https://www.tensorflow.org/datasets/catalog/c4), which was developed and released in the context of the same [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) as T5.\n",
       "\n",
       "The model was pre-trained on a on a multi-task mixture of unsupervised and supervised tasks. \n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This model is licensed under the following license: Apache 2.0</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/news/</h1><textarea rows=\"20\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "Topic Labeled News Dataset \n",
       "==========================================\n",
       "This data set was obtained from https://www.kaggle.com/kotartemiy/topic-labeled-news-dataset.  The source of the data is: the NewsCatcher team, who regularly collect and index news articles and release them to the open source community.\n",
       "\n",
       "\n",
       "=========================================\n",
       "Data Set Information\n",
       "=========================================\n",
       "https://www.kaggle.com/kotartemiy/topic-labeled-news-dataset. \n",
       "\n",
       "About the data collectors: https://newscatcherapi.com/. Contact them at team@newscatcherapi.com.\n",
       "\t\n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "\n",
       "This data set is licensed under the following license: CC0 1.0 Universal (CC0 1.0)\n",
       "Public Domain Dedication</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/poem_sentiment/</h1><textarea rows=\"44\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "poem_sentiment\n",
       "==========================================\n",
       "This data set was obtained from https://huggingface.co/datasets/poem_sentiment.  The source of the data is:\n",
       "Emily Sheng, David Uthus from https://arxiv.org/abs/2011.02686\n",
       "]\n",
       "\n",
       "=========================================\n",
       "Data Set Information\n",
       "=========================================\n",
       "Poem Sentiment is a sentiment dataset of poem verses from Project Gutenberg. This dataset can be used for tasks such as sentiment classification or style transfer for poems.\n",
       "\n",
       "Example of one instance in the dataset.\n",
       "\n",
       "{'id': 0, 'label': 2, 'verse_text': 'with pale blue berries. in these peaceful shades--'}\n",
       "\n",
       "Data Fields\n",
       "\n",
       "id: index of the example\n",
       "verse_text: The text of the poem verse\n",
       "label: The sentiment label.\n",
       "0 = negative\n",
       "1 = positive\n",
       "2 = no impact\n",
       "3 = mixed (both negative and positive)\n",
       "Note: The original dataset uses different label indices (negative = -1, no impact = 0, positive = 1)\n",
       "\n",
       "The dataset is split into a train, validation, and test split with the following sizes: 892, 105, 104 respectively\n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This data set is licensed under the following license: Creative Commons Attribution 4.0 International License\n",
       "\n",
       "@misc{sheng2020investigating,\n",
       "      title={Investigating Societal Biases in a Poetry Composition System},\n",
       "      author={Emily Sheng and David Uthus},\n",
       "      year={2020},\n",
       "      eprint={2011.02686},\n",
       "      archivePrefix={arXiv},\n",
       "      primaryClass={cs.CL}\n",
       "}\n",
       "</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/reviews/</h1><textarea rows=\"20\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "Fake Laptop Reviews\n",
       "==========================================\n",
       "\n",
       "This dataset was created by Databricks.\n",
       "\n",
       "=========================================\n",
       "Data Set Information\n",
       "=========================================\n",
       "\n",
       "This dataset includes a long form review plus several examples of smaller customer reviews.\n",
       "\n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "Copyright (2023) Databricks, Inc.\n",
       "This dataset is licensed under a Creative Commons Attribution 4.0 International License <https://creativecommons.org/licenses/by/4.0/>.\n",
       "\n",
       "</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/salaries/</h1><textarea rows=\"29\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "Data Science Salaries 2023\n",
       "==========================================\n",
       "\n",
       "This data set was obtained from https://www.kaggle.com/datasets/arnabchaki/data-science-salaries-2023.\n",
       "\n",
       "=========================================\n",
       "Data Set Information\n",
       "=========================================\n",
       "Data Science Job Salaries Dataset contains 11 columns, each are:\n",
       "\n",
       "work_year: The year the salary was paid.\n",
       "experience_level: The experience level in the job during the year\n",
       "employment_type: The type of employment for the role\n",
       "job_title: The role worked in during the year.\n",
       "salary: The total gross salary amount paid.\n",
       "salary_currency: The currency of the salary paid as an ISO 4217 currency code.\n",
       "salaryinusd: The salary in USD\n",
       "employee_residence: Employee's primary country of residence in during the work year as an ISO 3166 country code.\n",
       "remote_ratio: The overall amount of work done remotely\n",
       "company_location: The country of the employer's main office or contracting branch\n",
       "company_size: The median number of people that worked for the company during the year\n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This data set is licensed under the following license: Open Data Commons Open Database License\n",
       "\n",
       "</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/sentence-transformers_all-MiniLM-L6-v2/</h1><textarea rows=\"15\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "sentence-transformers/all-MiniLM-L6-v2\n",
       "==========================================\n",
       "The model was obtained from: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
       "\n",
       "=========================================\n",
       "Model Information\n",
       "=========================================\n",
       "This is a [sentence-transformers](https://www.sbert.net/) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This model is licensed under the following license: Apache 2.0</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/tokenizers/</h1><textarea rows=\"116\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "punkt by NLTK\n",
       "==========================================\n",
       "This data set was obtained from https://www.nltk.org/nltk_data/. \n",
       "\n",
       "=========================================\n",
       "Data Set Information\n",
       "=========================================\n",
       "\n",
       "The corpora contained about 400,000 tokens on average and mostly consisted of newspaper text converted to\n",
       "Unicode using the codecs module.\n",
       "\n",
       "Kiss, Tibor and Strunk, Jan (2006): Unsupervised Multilingual Sentence Boundary Detection.\n",
       "Computational Linguistics 32: 485-525.\n",
       "\n",
       "Pretrained Punkt Models -- Jan Strunk (New version trained after issues 313 and 514 had been corrected)\n",
       "\n",
       "Most models were prepared using the test corpora from Kiss and Strunk (2006). Additional models have\n",
       "been contributed by various people using NLTK for sentence boundary detection.\n",
       "\n",
       "For information about how to use these models, please confer the tokenization HOWTO:\n",
       "http://nltk.googlecode.com/svn/trunk/doc/howto/tokenize.html\n",
       "and chapter 3.8 of the NLTK book:\n",
       "http://nltk.googlecode.com/svn/trunk/doc/book/ch03.html#sec-segmentation\n",
       "\n",
       "There are pretrained tokenizers for the following languages:\n",
       "\n",
       "File                Language            Source                             Contents                Size of training corpus(in tokens)           Model contributed by\n",
       "=======================================================================================================================================================================\n",
       "czech.pickle        Czech               Multilingual Corpus 1 (ECI)        Lidove Noviny                   ~345,000                             Jan Strunk / Tibor Kiss\n",
       "                                                                           Literarni Noviny\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "danish.pickle       Danish              Avisdata CD-Rom Ver. 1.1. 1995     Berlingske Tidende              ~550,000                             Jan Strunk / Tibor Kiss\n",
       "                                        (Berlingske Avisdata, Copenhagen)  Weekend Avisen\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "dutch.pickle        Dutch               Multilingual Corpus 1 (ECI)        De Limburger                    ~340,000                             Jan Strunk / Tibor Kiss\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "english.pickle      English             Penn Treebank (LDC)                Wall Street Journal             ~469,000                             Jan Strunk / Tibor Kiss\n",
       "                    (American)\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "estonian.pickle     Estonian            University of Tartu, Estonia       Eesti Ekspress                  ~359,000                             Jan Strunk / Tibor Kiss\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "finnish.pickle      Finnish             Finnish Parole Corpus, Finnish     Books and major national        ~364,000                             Jan Strunk / Tibor Kiss\n",
       "                                        Text Bank (Suomen Kielen           newspapers\n",
       "                                        Tekstipankki)\n",
       "                                        Finnish Center for IT Science\n",
       "                                        (CSC)\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "french.pickle       French              Multilingual Corpus 1 (ECI)        Le Monde                        ~370,000                             Jan Strunk / Tibor Kiss\n",
       "                    (European)\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "german.pickle       German              Neue Zürcher Zeitung AG            Neue Zürcher Zeitung            ~847,000                             Jan Strunk / Tibor Kiss\n",
       "                    (Switzerland)       CD-ROM\n",
       "                    (Uses \"ss\"\n",
       "                     instead of \"ß\")\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "greek.pickle        Greek               Efstathios Stamatatos              To Vima (TO BHMA)               ~227,000                             Jan Strunk / Tibor Kiss\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "italian.pickle      Italian             Multilingual Corpus 1 (ECI)        La Stampa, Il Mattino           ~312,000                             Jan Strunk / Tibor Kiss\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "norwegian.pickle    Norwegian           Centre for Humanities              Bergens Tidende                 ~479,000                             Jan Strunk / Tibor Kiss\n",
       "                    (Bokmål and         Information Technologies,\n",
       "                     Nynorsk)           Bergen\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "polish.pickle       Polish              Polish National Corpus             Literature, newspapers, etc.  ~1,000,000                             Krzysztof Langner\n",
       "                                        (http://www.nkjp.pl/)\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "portuguese.pickle   Portuguese          CETENFolha Corpus                  Folha de São Paulo              ~321,000                             Jan Strunk / Tibor Kiss\n",
       "                    (Brazilian)         (Linguateca)\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "slovene.pickle      Slovene             TRACTOR                            Delo                            ~354,000                             Jan Strunk / Tibor Kiss\n",
       "                                        Slovene Academy for Arts\n",
       "                                        and Sciences\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "spanish.pickle      Spanish             Multilingual Corpus 1 (ECI)        Sur                             ~353,000                             Jan Strunk / Tibor Kiss\n",
       "                    (European)\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "swedish.pickle      Swedish             Multilingual Corpus 1 (ECI)        Dagens Nyheter                  ~339,000                             Jan Strunk / Tibor Kiss\n",
       "                                                                           (and some other texts)\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "turkish.pickle      Turkish             METU Turkish Corpus                Milliyet                        ~333,000                             Jan Strunk / Tibor Kiss\n",
       "                                        (Türkçe Derlem Projesi)\n",
       "                                        University of Ankara\n",
       "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "\n",
       "\n",
       "\n",
       "---- Training Code ----\n",
       "\n",
       "# import punkt\n",
       "import nltk.tokenize.punkt\n",
       "\n",
       "# Make a new Tokenizer\n",
       "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
       "\n",
       "# Read in training corpus (one example: Slovene)\n",
       "import codecs\n",
       "text = codecs.open(\"slovene.plain\",\"Ur\",\"iso-8859-2\").read()\n",
       "\n",
       "# Train tokenizer\n",
       "tokenizer.train(text)\n",
       "\n",
       "# Dump pickled tokenizer\n",
       "import pickle\n",
       "out = open(\"slovene.pickle\",\"wb\")\n",
       "pickle.dump(tokenizer, out)\n",
       "out.close()\n",
       "\n",
       "---------\n",
       "\n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This data set is licensed under the following license: Apache 2.0</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/wiki_bio/</h1><textarea rows=\"101\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "wiki_bio\n",
       "==========================================\n",
       "[insert source information, including URL, e.g. for http://archive.ics.uci.edu/ml/datasets/Online+Retail, put:\n",
       "This data set was obtained from http://archive.ics.uci.edu/ml/datasets/Online+Retail.  The source of the data is:\n",
       "Dr Daqing Chen, Director: Public Analytics group. chend '@' lsbu.ac.uk, School of Engineering, London South Bank University, London SE1 0AA, UK.\n",
       "]\n",
       "\n",
       "=========================================\n",
       "Data Set Information\n",
       "=========================================\n",
       "This Dataset contains 728321 biographies extracted from Wikipedia containing the first paragraph of the biography and the tabular infobox.\n",
       "\n",
       "**Supported Tasks and Leaderboards**\n",
       "The main purpose of this dataset is developing text generation models.\n",
       "\n",
       "**Languages**\n",
       "English.\n",
       "\n",
       "The structure of a single sample is the following:\n",
       "```\n",
       "{\n",
       "   \"input_text\":{\n",
       "      \"context\":\"pope michael iii of alexandria\\n\",\n",
       "      \"table\":{\n",
       "         \"column_header\":[\n",
       "            \"type\",\n",
       "            \"ended\",\n",
       "            \"death_date\",\n",
       "            \"title\",\n",
       "            \"enthroned\",\n",
       "            \"name\",\n",
       "            \"buried\",\n",
       "            \"religion\",\n",
       "            \"predecessor\",\n",
       "            \"nationality\",\n",
       "            \"article_title\",\n",
       "            \"feast_day\",\n",
       "            \"birth_place\",\n",
       "            \"residence\",\n",
       "            \"successor\"\n",
       "         ],\n",
       "         \"content\":[\n",
       "            \"pope\",\n",
       "            \"16 march 907\",\n",
       "            \"16 march 907\",\n",
       "            \"56th of st. mark pope of alexandria & patriarch of the see\",\n",
       "            \"25 april 880\",\n",
       "            \"michael iii of alexandria\",\n",
       "            \"monastery of saint macarius the great\",\n",
       "            \"coptic orthodox christian\",\n",
       "            \"shenouda i\",\n",
       "            \"egyptian\",\n",
       "            \"pope michael iii of alexandria\\n\",\n",
       "            \"16 -rrb- march -lrb- 20 baramhat in the coptic calendar\",\n",
       "            \"egypt\",\n",
       "            \"saint mark 's church\",\n",
       "            \"gabriel i\"\n",
       "         ],\n",
       "         \"row_number\":[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
       "      }\n",
       "   },\n",
       "   \"target_text\":\"pope michael iii of alexandria -lrb- also known as khail iii -rrb- was the coptic pope of alexandria and patriarch of the see of st. mark -lrb- 880 -- 907 -rrb- .\\nin 882 , the governor of egypt , ahmad ibn tulun , forced khail to pay heavy contributions , forcing him to sell a church and some attached properties to the local jewish community .\\nthis building was at one time believed to have later become the site of the cairo geniza .\\n\"\n",
       "}\n",
       "\n",
       "```\n",
       "where, in the \"table\" field, all the information of the Wikpedia infobox is stored (the header of the infobox is stored in \"column_header\" and the information in the \"content\" field).\n",
       "\n",
       "Data Splits\n",
       "- Train: 582659 samples.\n",
       "- Test: 72831 samples.\n",
       "- Validation: 72831 samples\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This data set is licensed under the following license: Creative Commons CC BY-SA 3.0 License.\n",
       "\n",
       "This dataset was announced in the paper Neural Text Generation from Structured Data with Application to the Biography Domain (arxiv link) and is stored in this repo (owned by DavidGrangier).\n",
       "\n",
       "@article{DBLP:journals/corr/LebretGA16,\n",
       "  author    = {R{\\'{e}}mi Lebret and\n",
       "               David Grangier and\n",
       "               Michael Auli},\n",
       "  title     = {Generating Text from Structured Data with Application to the Biography\n",
       "               Domain},\n",
       "  journal   = {CoRR},\n",
       "  volume    = {abs/1603.07771},\n",
       "  year      = {2016},\n",
       "  url       = {http://arxiv.org/abs/1603.07771},\n",
       "  archivePrefix = {arXiv},\n",
       "  eprint    = {1603.07771},\n",
       "  timestamp = {Mon, 13 Aug 2018 16:48:30 +0200},\n",
       "  biburl    = {https://dblp.org/rec/journals/corr/LebretGA16.bib},\n",
       "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
       "}\n",
       "</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html><body><h1>dbfs:/mnt/dbacademy-users/johnlennyt@gmail.com/large-language-models/datasets/xsum/</h1><textarea rows=\"44\" style=\"width:100%; overflow-x:scroll; white-space:nowrap\">==========================================\n",
       "xsum\n",
       "==========================================\n",
       "This data set was obtained from https://huggingface.co/datasets/xsum.  The source of the data is:\n",
       "Shashi Narayan, Shay B. Cohen, Mirella Lapata from https://arxiv.org/abs/1808.08745.\n",
       "]\n",
       "\n",
       "=========================================\n",
       "Data Set Information\n",
       "=========================================\n",
       "Extreme Summarization (XSum) Dataset.\n",
       "\n",
       "There are three features:\n",
       "\n",
       "document: string: Input news article.\n",
       "summary: string: One sentence summary of the article.\n",
       "id: string: BBC ID of the article.\n",
       "\n",
       "An example of 'validation' looks as follows. The data fields are the same among all splits.\n",
       "{\n",
       "    \"document\": \"some-body\",\n",
       "    \"id\": \"29750031\",\n",
       "    \"summary\": \"some-sentence\"\n",
       "}\n",
       "\n",
       "train: 204,045 samples \n",
       "validation: 11,332 samples\n",
       "test: 11,334 samples\n",
       "\n",
       "    \n",
       "=========================================\n",
       "License and/or Citation\n",
       "=========================================\n",
       "This data set is licensed under the following license: MIT\n",
       "\n",
       "@article{Narayan2018DontGM,\n",
       "  title={Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization},\n",
       "  author={Shashi Narayan and Shay B. Cohen and Mirella Lapata},\n",
       "  journal={ArXiv},\n",
       "  year={2018},\n",
       "  volume={abs/1808.08745}\n",
       "}\n",
       "</textarea></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Once initialized, just print the copyrights\n",
    "DA.print_copyrights()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Print-Dataset-Copyrights",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
